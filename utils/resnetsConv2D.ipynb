{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7R0yO1XdBNh"
   },
   "source": [
    "### Démarrage de tensorboard et imports principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_DISABLE_MKL'] = '1'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCCbrXXRuvi",
    "outputId": "7d91c098-39d8-4a89-adf8-3133ab8217de"
   },
   "outputs": [],
   "source": [
    "# Agrandir le notebook ?\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Démarrage de tensorboard pour notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil  \n",
    "from math import ceil, floor\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.framework import ops #pour tenter de reset tensorboard, sans grand succès\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF9xVI2bdzuT"
   },
   "source": [
    "### Hyper paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametres de verification : \n",
    "\n",
    "list_indiv_id = ['1', '2']\n",
    "list_epochs = [50, 50]\n",
    "list_batch_size = [320, 350]\n",
    "list_blocks_size = [2, 2]\n",
    "list_blocks_nb = [16, 16]\n",
    "list_l1 = [0, 0]\n",
    "list_l2 = [0, 0]\n",
    "list_batch_norm = [0, 0]\n",
    "list_dropout = [0, 0]\n",
    "list_filters_per_layers = [64, 64]\n",
    "list_filters_double = [4, 3] #défini quand doubler cette valeur\n",
    "list_filters_pool = [4, 3] #défini tout les combien de block est effectué une couche de pooling, ne peut être nul\n",
    "list_activation = ['relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3)]\n",
    "list_first_kernel = [(7,7), (7,7)]\n",
    "list_padding = ['same', 'same']\n",
    "list_max_or_avg_pool = ['avg', 'avg']\n",
    "list_learning_r = [0.001, 0.001]\n",
    "list_momentum = [0.9, 0.9]\n",
    "list_optimizer = ['Adam', 'Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\resnetsConv2d\\\\logs_\" + current_time\n",
    "\n",
    "\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIIDw6pHdJbK"
   },
   "source": [
    "# Fonctions pour préparer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tSU6Hl2Ruv0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- annotations 1030747\n",
      "- categories 32094\n",
      "- images 1030747\n",
      "- info 6\n",
      "- licenses 1\n",
      "- regions 4\n",
      "================================\n",
      "- images 138292\n",
      "- info 6\n",
      "- licenses 1\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    DIR_TEST = 'D:/Projets/DL_4A_2nd/dataset/nybg2020/test/'\n",
    "    DIR_TRAIN = 'D:/Projets/DL_4A_2nd/dataset/nybg2020/train/'\n",
    "    META_DATA_TRAIN = DIR_TRAIN+'metadata.json'\n",
    "    META_DATA_TEST = DIR_TEST+'metadata.json'\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with open(META_DATA_TRAIN, 'r') as json_file:\n",
    "        data_train = json.load(json_file)\n",
    "        for key in data_train:\n",
    "            print(\"-\",key, len(data_train[key]))\n",
    "    print(\"================================\")\n",
    "    with open(META_DATA_TEST, 'r') as json_file:\n",
    "        data_test = json.load(json_file)\n",
    "        for key in data_test:\n",
    "            print(\"-\",key, len(data_test[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):    \n",
    "    df_images = pd.DataFrame(data_train['images'])\n",
    "    df_categories = pd.DataFrame(data_train['categories'])\n",
    "    df_annotations = pd.DataFrame(data_train['annotations'])\n",
    "    assert len(df_annotations) == len(df_images)\n",
    "    df_images_annotations = pd.merge(df_images, df_annotations, left_on='id', right_on='image_id', how='right').drop('image_id', axis=1)\n",
    "    df_images_annotations = df_images_annotations.sort_values(['category_id'])\n",
    "    \n",
    "    df_families = df_categories.drop('genus', axis=1).drop('id', axis=1).drop('name', axis=1).drop_duplicates().reset_index().drop('index', axis=1)\n",
    "    df_families.head(3)\n",
    "    \n",
    "    df_images_infos = pd.merge(df_images_annotations, df_categories, left_on='category_id', right_on='id', how='right')\\\n",
    "                    .drop('height', axis=1).drop('id_x', axis=1).drop('license', axis=1).drop('width', axis=1)\\\n",
    "                    .drop('category_id', axis=1).drop('id_y', axis=1).drop('region_id', axis=1).drop('name', axis=1)\n",
    "\n",
    "    df_images_infos = df_images_infos.rename(columns={\"id\":\"category_id\"})\n",
    "\n",
    "    df_images_infos.sort_values(by='family', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    df_images = pd.DataFrame(data_test['images']).drop('height', axis=1).drop('width', axis=1).drop('license', axis=1)\n",
    "    df_images.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):    \n",
    "    df_images_infos.to_csv('D:/Projets/DL_4A_2nd/full_train_data.csv', index=False)\n",
    "    df_images.to_csv('D:/Projets/DL_4A_2nd/full_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLORE DATA / DATA GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3680.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>280.094565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>763.668945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>198.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14490.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category_id\n",
       "count   3680.000000\n",
       "mean     280.094565\n",
       "std      763.668945\n",
       "min        2.000000\n",
       "25%       12.000000\n",
       "50%       54.000000\n",
       "75%      198.000000\n",
       "max    14490.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/000/00/626762.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/000/00/72077.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/000/00/818271.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/000/00/495523.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/000/00/437000.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030743</th>\n",
       "      <td>images/320/93/946506.jpg</td>\n",
       "      <td>41</td>\n",
       "      <td>3677</td>\n",
       "      <td>32093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030744</th>\n",
       "      <td>images/320/93/1010199.jpg</td>\n",
       "      <td>41</td>\n",
       "      <td>3677</td>\n",
       "      <td>32093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030745</th>\n",
       "      <td>images/320/93/35450.jpg</td>\n",
       "      <td>41</td>\n",
       "      <td>3677</td>\n",
       "      <td>32093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030746</th>\n",
       "      <td>images/320/93/68259.jpg</td>\n",
       "      <td>41</td>\n",
       "      <td>3677</td>\n",
       "      <td>32093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030747</th>\n",
       "      <td>NaN</td>\n",
       "      <td>239</td>\n",
       "      <td>2663</td>\n",
       "      <td>23079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030748 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file_name  family  genus  category_id\n",
       "0         images/000/00/626762.jpg       0      0            0\n",
       "1          images/000/00/72077.jpg       0      0            0\n",
       "2         images/000/00/818271.jpg       0      0            0\n",
       "3         images/000/00/495523.jpg       0      0            0\n",
       "4         images/000/00/437000.jpg       0      0            0\n",
       "...                            ...     ...    ...          ...\n",
       "1030743   images/320/93/946506.jpg      41   3677        32093\n",
       "1030744  images/320/93/1010199.jpg      41   3677        32093\n",
       "1030745    images/320/93/35450.jpg      41   3677        32093\n",
       "1030746    images/320/93/68259.jpg      41   3677        32093\n",
       "1030747                        NaN     239   2663        23079\n",
       "\n",
       "[1030748 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    family = df_images_infos[['family', 'genus', 'category_id']].groupby(['family', 'genus']).count()\n",
    "    display(family.describe())\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                                         featurewise_std_normalization=False,\n",
    "                                         rotation_range=180,\n",
    "                                         width_shift_range=0.1,\n",
    "                                         height_shift_range=0.1,\n",
    "                                         zoom_range=0.2)\n",
    "    \n",
    "    m = df_images_infos[['file_name', 'family', 'genus', 'category_id']]\n",
    "    fam = m.family.unique().tolist()\n",
    "    m.family = m.family.map(lambda x: fam.index(x))\n",
    "    gen = m.genus.unique().tolist()\n",
    "    m.genus = m.genus.map(lambda x: gen.index(x))\n",
    "    display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORMAT DATA AND SELECT TRAIN/TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KILL THE NAN VALUE IN M TO PREVENT BUGS\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(\"KILL THE NAN VALUE IN M TO PREVENT BUGS\")\n",
    "print(type(m))\n",
    "na = m.file_name.isna()\n",
    "keep = [x for x in range(m.shape[0]) if not na[x]]\n",
    "m = m.iloc[keep]\n",
    "\n",
    "train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)\n",
    "train = train[:800000]\n",
    "verif = verif[:200000]\n",
    "shape = (64, 64, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTdXP2D-c2D5"
   },
   "source": [
    "# Classe Python pour définir les individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jP0CrTpRuv8"
   },
   "outputs": [],
   "source": [
    "# Classe pour les convnets\n",
    "class IndividuResnetConv2d:\n",
    "    def __init__(self, indiv_id='1', epochs=10, batch_size=1, block_size=1, block_nb=1, l1=0, l2=0, batch_norm=0, dropout=0, filters_per_layers=64, filters_double=3, filters_pool=1, activation='relu', kernel=(3,3), first_kernel=(7,7), padding='same', max_or_avg_pool=0, learning_r=0.01, momentum=0.9, optimizer='SGD'):\n",
    "        # Initialisation de nos variables\n",
    "        self.time_fit = datetime.datetime.now()\n",
    "        self.my_reguralizer = None\n",
    "        \n",
    "        if block_size < 1:\n",
    "            self.block_size = 1\n",
    "        else:\n",
    "            self.block_size = block_size\n",
    "            \n",
    "        if block_nb < 2:\n",
    "            self.block_nb = 1\n",
    "        else:\n",
    "            self.block_nb = block_nb\n",
    "            \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.indiv_id = indiv_id\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.filters_per_layers = filters_per_layers\n",
    "        \n",
    "        # Création d'un variable qui va garder la valeur de filters_per_layers (elle changera dans le modèle)\n",
    "        self.keep_filters_per_layers = filters_per_layers\n",
    "        \n",
    "        if filters_double < 2:\n",
    "            self.filters_double = 2\n",
    "        else : \n",
    "            self.filters_double = filters_double\n",
    "            \n",
    "        if filters_pool < 1:\n",
    "            self.filters_pool = 1\n",
    "        else : \n",
    "            self.filters_pool = filters_pool\n",
    "        \n",
    "        \n",
    "        self.activation = activation\n",
    "        self.kernel = kernel\n",
    "        self.first_kernel = first_kernel\n",
    "        self.padding = padding\n",
    "        self.max_or_avg_pool = max_or_avg_pool\n",
    "        self.learning_r = learning_r\n",
    "        self.momentum = momentum\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # ToString()\n",
    "    def __str__(self):\n",
    "        ma_liste = []\n",
    "        ma_liste.append(\"indiv_id:{},\\n \".format(self.indiv_id))\n",
    "        ma_liste.append(\"epochs:{},\\n \".format(self.epochs))\n",
    "        ma_liste.append(\"batch_size:{},\\n \".format(self.batch_size))\n",
    "        ma_liste.append(\"block_size:{},\\n \".format(self.block_size))\n",
    "        ma_liste.append(\"block_nb:{},\\n \".format(self.block_nb))\n",
    "        ma_liste.append(\"l1:{},\\n \".format(self.l1))\n",
    "        ma_liste.append(\"l2:{},\\n \".format(self.l2))\n",
    "        ma_liste.append(\"batch_norm:{},\\n \".format(self.batch_norm))\n",
    "        ma_liste.append(\"dropout:{},\\n \".format(self.dropout))\n",
    "        ma_liste.append(\"filters_per_layers:{},\\n \".format(self.filters_per_layers))\n",
    "        ma_liste.append(\"filters_double:{},\\n \".format(self.filters_double))\n",
    "        ma_liste.append(\"filters_pool:{},\\n \".format(self.filters_pool))\n",
    "        ma_liste.append(\"activation:{},\\n \".format(self.activation))\n",
    "        ma_liste.append(\"kernel:\\n \")\n",
    "        ma_liste.append(\"{},\\n \".format(self.kernel))\n",
    "        ma_liste.append(\"first_kernel:\\n \")\n",
    "        ma_liste.append(\"{},\\n \".format(self.first_kernel))\n",
    "        ma_liste.append(\"padding:{},\\n \".format(self.padding))\n",
    "        ma_liste.append(\"max_or_avg_pool:{}\\n\".format(self.max_or_avg_pool))\n",
    "        ma_liste.append(\"learning_r:{}\\n\".format(self.learning_r))\n",
    "        ma_liste.append(\"momentum:{}\\n\".format(self.momentum))\n",
    "        ma_liste.append(\"optimizer:{}\\n\".format(self.optimizer))\n",
    "            \n",
    "        return ma_liste\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # (Modele 2 conv + norm ? + pool) * X -> MLP -> softmax sortie 10 -> MODELE BLOC 2\n",
    "    # D'autres modeles seront crees par la suite\n",
    "    def create_model(self, main_directory, shape):\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        # Choix d'un emplacement pour les logs\n",
    "        log_dir=main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\tensorboard_data\\\\\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        print(\"log dir = \",log_dir)\n",
    "        \n",
    "        # l1 et l2\n",
    "        if self.l1 > 0 and self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l1_l2(l1=self.l1 / (self.block_nb*self.block_size), l2=self.l2 / (self.block_nb*self.block_size))\n",
    "        if self.l1 > 0:\n",
    "            self.my_regularizer = regularizers.l1(self.l1 / (self.block_nb*self.block_size))\n",
    "        elif self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l2(self.l2 / (self.block_nb*self.block_size))\n",
    "        else:\n",
    "            self.my_reguralizer = None\n",
    "            \n",
    "        \n",
    "\n",
    "        # Faire toutes les convs nécessaires (conv * 2 + max pool)\n",
    "        double_count = 0 # Var pour doubler les filtres\n",
    "        \n",
    "        input_layer = Input(shape)\n",
    "        count = 1\n",
    "        count2 = 1\n",
    "\n",
    "        last_output = Conv2D(self.filters_per_layers, self.first_kernel, padding=self.padding, input_shape=(32, 32, 3), name='conv_'+str(self.filters_per_layers)+'_0',\n",
    "                      kernel_regularizer=self.my_reguralizer)(input_layer)\n",
    "        last_output = Activation(activation=self.activation, name=\"Activation_0\")(last_output)\n",
    "\n",
    "        if self.max_or_avg_pool == 'max':\n",
    "            last_output = MaxPooling2D((2, 2), padding=self.padding, name='max_pool_'+str(0))(last_output)\n",
    "        else:\n",
    "            last_output = AveragePooling2D((2, 2), padding=self.padding, name='avg_pool_'+str(0))(last_output)\n",
    "\n",
    "        for i in range(self.block_nb):\n",
    "\n",
    "            block_input = last_output\n",
    "            for j in range(self.block_size):\n",
    "                last_output = Conv2D(self.filters_per_layers, self.kernel, padding=self.padding, input_shape=(32, 32, 3), name='conv_'+str(self.filters_per_layers)+'_'+str(count),\n",
    "                          kernel_regularizer=self.my_reguralizer)(last_output)\n",
    "                last_output = Activation(activation=self.activation, name=f\"Activation_{count}\")(last_output)\n",
    "                if self.batch_norm == 1:\n",
    "                    last_output = BatchNormalization(name='batchnorm_'+str(count))(last_output)\n",
    "                count += 1\n",
    "                \n",
    "            \n",
    "            block_input = Conv2D(self.filters_per_layers, self.kernel, padding=self.padding, input_shape=(32, 32, 3), name='conv_identity'+str(self.filters_per_layers)+'_'+str(i)\n",
    "                          )(block_input)\n",
    "\n",
    "            last_output = Add(name=f\"Add_output_{i}\")([last_output, block_input])\n",
    "            if(i%self.filters_pool == 0):\n",
    "                if self.max_or_avg_pool == 'max':\n",
    "                    last_output = MaxPooling2D((2, 2), padding=self.padding, name='max_pool_'+str(count2))(last_output)\n",
    "                else:\n",
    "                    last_output = AveragePooling2D((2, 2), padding=self.padding, name='avg_pool_'+str(count2))(last_output)\n",
    "                count2 += 1\n",
    "\n",
    "            if(i % self.filters_double == 0):\n",
    "                self.filters_per_layers = self.filters_per_layers * 2\n",
    "            if self.dropout > 0:\n",
    "                last_output = Dropout(self.dropout)(last_output)\n",
    "            \n",
    "\n",
    "        flatten_layer_output = Flatten(name=\"flatten\")(last_output)  \n",
    "        \n",
    "        last_output = Dense(128, activation='relu', kernel_regularizer=self.my_reguralizer,\n",
    "                            name='mlp_end')(flatten_layer_output)\n",
    "        if self.batch_norm == 1:\n",
    "                    last_output = BatchNormalization(name='batchnorm_end')(last_output)\n",
    "        if self.dropout > 0:\n",
    "            last_output = Dropout(self.dropout)(last_output)         \n",
    "                        \n",
    "        \n",
    "        output_tensor = Dense(32094, activation=softmax, name=f\"Dense_output\")(last_output)\n",
    "\n",
    "        return Model(input_layer, output_tensor)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def train_model(self, train, verif, model):\n",
    "        # Compiler le modele\n",
    "        if self.optimizer == 'SGD':\n",
    "            print(\"SGD, learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = SGD(lr=self.learning_r, momentum=self.momentum)\n",
    "        else:\n",
    "            print(\"Adam learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = Adam(lr=self.learning_r, beta_1=self.momentum) # beta_1 => notation pour momentum Adam\n",
    "        model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Entrainer le modele\n",
    "        #history = model.fit(trainX, trainY, epochs=self.epochs, batch_size=self.batch_size, \n",
    "        #                    validation_data=(testX, testY), verbose=1, callbacks=[tensorboard_callback])\n",
    "        \n",
    "        history = model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,\n",
    "                                                      directory=DIR_TRAIN,\n",
    "                                                      x_col=\"file_name\",\n",
    "                                                      y_col=[\"category_id\"],\n",
    "                                                      target_size=(64, 64),\n",
    "                                                      batch_size=self.batch_size,\n",
    "                                                      class_mode='raw'),\n",
    "                    validation_data=train_datagen.flow_from_dataframe(\n",
    "                        dataframe=verif,\n",
    "                        directory=DIR_TRAIN,\n",
    "                        x_col=\"file_name\",\n",
    "                        y_col=[\"category_id\"],\n",
    "                        target_size=(64, 64),\n",
    "                        batch_size=self.batch_size,\n",
    "                        class_mode='raw'),\n",
    "                    epochs=self.epochs,\n",
    "                    steps_per_epoch=len(train)//self.batch_size,\n",
    "                    validation_steps=len(verif)//self.batch_size,\n",
    "                    verbose=1,\n",
    "                    workers=8,\n",
    "                    use_multiprocessing=False)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Garder une trace du temps nécessaire pour fit (peut être pas la meilleure méthode)\n",
    "        end = datetime.datetime.now()\n",
    "        self.time_fit = end - start\n",
    "        print(\"\\nTime for fit = \", round(self.time_fit.total_seconds(),2)) # Round avec total_seconds()\n",
    "        \n",
    "        #Arpès que le fit soit fait, remettre filters_per_layers à sa valeur initiale pour un meilleur log \n",
    "        self.filters_per_layers = self.keep_filters_per_layers\n",
    "\n",
    "        return history, model\n",
    "    \n",
    "    \n",
    "    def save_model(self, history, model, main_directory, current_time):\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        plot_model(model, \"model.png\")\n",
    "        \n",
    "        # Deplacement modele au bon endroit\n",
    "        shutil.move(os.getcwd()+\"\\\\model.png\", main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\model.png\")\n",
    "        \n",
    "        # Afficher nos résultats dans un graphique matplotlib sauvegardé\n",
    "        pyplot.gcf().subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # Afficher la loss\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "        \n",
    "        # Afficher l'accuracy\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "        \n",
    "        # Sauvegarde\n",
    "        filename = main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\\"\n",
    "        pyplot.savefig(filename + 'plot.png')\n",
    "        pyplot.close()\n",
    "       \n",
    "        \n",
    "        print(\"LOSS : \", round(history.history['loss'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_LOSS : \", round(history.history['val_loss'][self.epochs-1].item(), 3))\n",
    "        print(\"ACCURACY : \", round(history.history['accuracy'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_ACCURACY : \", round(history.history['val_accuracy'][self.epochs-1].item(), 3))\n",
    "        \n",
    "        # attributs pour créer les csv indivudels et le csv global\n",
    "        self.loss = round(history.history['loss'][self.epochs-1].item(), 3)\n",
    "        self.val_loss = round(history.history['val_loss'][self.epochs-1].item(), 3)\n",
    "        self.accuracy = round(history.history['accuracy'][self.epochs-1].item(), 3)\n",
    "        self.val_accuracy = round(history.history['val_accuracy'][self.epochs-1].item(), 3)\n",
    "        self.time_taken = round(self.time_fit.total_seconds(),2)\n",
    "        \n",
    "        # Créer un dataframe pandas (avec hyperparams) et le sauvegarder en CSV\n",
    "        df = pd.DataFrame({'indiv_id': [self.indiv_id],\n",
    "                           'epochs': [self.epochs],\n",
    "                           'batch_size': [self.batch_size],\n",
    "                           'block_size': [self.block_size],\n",
    "                           'block_nb': [self.block_nb],\n",
    "                           'l1': [self.l1],\n",
    "                           'l2': [self.l2],\n",
    "                           'batch_norm': [self.batch_norm],\n",
    "                           'dropout': [self.dropout],\n",
    "                           'filters_per_layers': [self.filters_per_layers],\n",
    "                           'filters_double': [self.filters_double],\n",
    "                           'filters_pool': [self.filters_pool],\n",
    "                           'activation': [self.activation],\n",
    "                           'kernel': [self.kernel],\n",
    "                           'first_kernel': [self.first_kernel],\n",
    "                           'padding': [self.padding],\n",
    "                           'max_or_avg_pool': [self.max_or_avg_pool],\n",
    "                           'loss': [self.loss],\n",
    "                           'val_loss': [self.val_loss],\n",
    "                           'accuracy': [self.accuracy],\n",
    "                           'val_accuracy': [self.val_accuracy],\n",
    "                           'time_taken' : [self.time_taken],\n",
    "                           'learning_r' : [self.learning_r],\n",
    "                           'momentum' : [self.momentum],\n",
    "                           'optimizer' : [self.optimizer]\n",
    "                          })\n",
    "        \n",
    "        df.to_csv(path_or_buf=filename+\"recap.csv\",index=False)\n",
    "    \n",
    "    # Lance toutes les étapes\n",
    "    def exec_indiv(self, main_directory, current_time):\n",
    "        \n",
    "        # Charger les données\n",
    "        #trainX, trainY, testX, testY = load_dataset()\n",
    "        \n",
    "        # Normaliser les données\n",
    "        #trainX, testX = prep_pixels(trainX, testX)\n",
    "        \n",
    "        # Créer et entrainer le modele\n",
    "        model = self.create_model(main_directory, shape)\n",
    "        print(model.summary())\n",
    "        model, model_before_train = self.train_model(train, verif, model)\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        #save = self.save_model(history, model, main_directory, current_time\n",
    "        \n",
    "        model.save('fg_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buU7Dsqfs6Cv"
   },
   "source": [
    "### Classe Python qui va démarrer les tests des neural nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im542kkes6Cx"
   },
   "outputs": [],
   "source": [
    "# Classe générale qui va nous servir à effectuer des actions sur des individus\n",
    "class MyTraining:\n",
    "    # Prends un ID et une liste d'individus \n",
    "    def __init__(self, id_train, indiv_list):\n",
    "        \n",
    "        self.id_train = id_train\n",
    "        self.indiv_list = indiv_list\n",
    "    \n",
    "    def train(self, main_directory, current_time):\n",
    "        \n",
    "        print(\"Start training\\n\")\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            print(\"indiv \", indiv.indiv_id, \"\\n\")\n",
    "            indiv.exec_indiv(main_directory, current_time)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "        '''\n",
    "        # Fusion des csv \n",
    "        merge_csv = pd.DataFrame(columns=['indiv_id', 'epochs', 'block_size', 'block_nb', 'l1', 'l2', 'batch_norm', 'dropout',\n",
    "                                          'filters_per_layers', 'filters_double', 'filters_pool', 'activation', 'kernel', \n",
    "                                          'first_kernel', 'padding','max_or_avg_pool','loss', 'val_loss', 'accuracy', 'val_accuracy',\n",
    "                                          'time_taken','learning_r', 'momentum', 'optimizer'])\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            merge_csv = merge_csv.append({'indiv_id': indiv.indiv_id, 'epochs': indiv.epochs, 'batch_size': indiv.batch_size, 'block_size' : indiv.block_size,\n",
    "                                          'block_nb' : indiv.block_nb, 'l1' : indiv.l1, 'l2' : indiv.l2, 'batch_norm': indiv.batch_norm, 'dropout' : indiv.dropout,\n",
    "                                          'filters_per_layers' : indiv.filters_per_layers, 'filters_double' : indiv.filters_double, 'filters_pool' : indiv.filters_pool, \n",
    "                                          'activation' : indiv.activation, 'kernel' : indiv.kernel, 'first_kernel' : indiv.first_kernel, 'padding' : indiv.padding, \n",
    "                                          'max_or_avg_pool' : indiv.max_or_avg_pool, 'loss' : indiv.loss, 'val_loss' : indiv.val_loss, 'accuracy' : indiv.accuracy, \n",
    "                                          'val_accuracy' : indiv.val_accuracy, 'time_taken' : indiv.time_taken, 'learning_r' : indiv.learning_r,\n",
    "                                          'momentum': indiv.momentum, 'optimizer' : indiv.optimizer\n",
    "                                         }, ignore_index=True)\n",
    "        \n",
    "        # sauvegarde\n",
    "        merge_csv.to_csv(main_directory+\"\\\\combined_recap.csv\", index=False)\n",
    "        '''    \n",
    "    \n",
    "    def all_indiv(self):\n",
    "        \n",
    "        # Affiche les caractéristiques de l'ensemble des individus\n",
    "        for indiv in self.indiv_list:\n",
    "            print('\\n'.join(indiv.__str__()))\n",
    "            for tir in range(80): print('-', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIGBnz_4bj74"
   },
   "source": [
    "### Traitement général (train de l'ensemble des modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjIV_jF6RuwF",
    "outputId": "6759a6e1-3664-4264-c87d-18266651af09",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "\n",
      "indiv  1 \n",
      "\n",
      "log dir =  D:\\Projets\\DL_4A_2nd\\HERBARIUM\\utils\\logs\\resnetsConv2d\\logs_2020-05-06-09-28\\log_1\\tensorboard_data\\\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_64_0 (Conv2D)              (None, 64, 64, 64)   9472        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Activation_0 (Activation)       (None, 64, 64, 64)   0           conv_64_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool_0 (AveragePooling2D)   (None, 32, 32, 64)   0           Activation_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_64_1 (Conv2D)              (None, 32, 32, 64)   36928       avg_pool_0[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Activation_1 (Activation)       (None, 32, 32, 64)   0           conv_64_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_64_2 (Conv2D)              (None, 32, 32, 64)   36928       Activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_2 (Activation)       (None, 32, 32, 64)   0           conv_64_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity64_0 (Conv2D)      (None, 32, 32, 64)   36928       avg_pool_0[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_0 (Add)              (None, 32, 32, 64)   0           Activation_2[0][0]               \n",
      "                                                                 conv_identity64_0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool_1 (AveragePooling2D)   (None, 16, 16, 64)   0           Add_output_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_128_3 (Conv2D)             (None, 16, 16, 128)  73856       avg_pool_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Activation_3 (Activation)       (None, 16, 16, 128)  0           conv_128_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_128_4 (Conv2D)             (None, 16, 16, 128)  147584      Activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_4 (Activation)       (None, 16, 16, 128)  0           conv_128_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity128_1 (Conv2D)     (None, 16, 16, 128)  73856       avg_pool_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_1 (Add)              (None, 16, 16, 128)  0           Activation_4[0][0]               \n",
      "                                                                 conv_identity128_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_128_5 (Conv2D)             (None, 16, 16, 128)  147584      Add_output_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_5 (Activation)       (None, 16, 16, 128)  0           conv_128_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_128_6 (Conv2D)             (None, 16, 16, 128)  147584      Activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_6 (Activation)       (None, 16, 16, 128)  0           conv_128_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity128_2 (Conv2D)     (None, 16, 16, 128)  147584      Add_output_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_2 (Add)              (None, 16, 16, 128)  0           Activation_6[0][0]               \n",
      "                                                                 conv_identity128_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_128_7 (Conv2D)             (None, 16, 16, 128)  147584      Add_output_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_7 (Activation)       (None, 16, 16, 128)  0           conv_128_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_128_8 (Conv2D)             (None, 16, 16, 128)  147584      Activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_8 (Activation)       (None, 16, 16, 128)  0           conv_128_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity128_3 (Conv2D)     (None, 16, 16, 128)  147584      Add_output_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_3 (Add)              (None, 16, 16, 128)  0           Activation_8[0][0]               \n",
      "                                                                 conv_identity128_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_128_9 (Conv2D)             (None, 16, 16, 128)  147584      Add_output_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_9 (Activation)       (None, 16, 16, 128)  0           conv_128_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_128_10 (Conv2D)            (None, 16, 16, 128)  147584      Activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_10 (Activation)      (None, 16, 16, 128)  0           conv_128_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity128_4 (Conv2D)     (None, 16, 16, 128)  147584      Add_output_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_4 (Add)              (None, 16, 16, 128)  0           Activation_10[0][0]              \n",
      "                                                                 conv_identity128_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool_2 (AveragePooling2D)   (None, 8, 8, 128)    0           Add_output_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_256_11 (Conv2D)            (None, 8, 8, 256)    295168      avg_pool_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Activation_11 (Activation)      (None, 8, 8, 256)    0           conv_256_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_256_12 (Conv2D)            (None, 8, 8, 256)    590080      Activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_12 (Activation)      (None, 8, 8, 256)    0           conv_256_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity256_5 (Conv2D)     (None, 8, 8, 256)    295168      avg_pool_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_5 (Add)              (None, 8, 8, 256)    0           Activation_12[0][0]              \n",
      "                                                                 conv_identity256_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_256_13 (Conv2D)            (None, 8, 8, 256)    590080      Add_output_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_13 (Activation)      (None, 8, 8, 256)    0           conv_256_13[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_256_14 (Conv2D)            (None, 8, 8, 256)    590080      Activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_14 (Activation)      (None, 8, 8, 256)    0           conv_256_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity256_6 (Conv2D)     (None, 8, 8, 256)    590080      Add_output_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_6 (Add)              (None, 8, 8, 256)    0           Activation_14[0][0]              \n",
      "                                                                 conv_identity256_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_256_15 (Conv2D)            (None, 8, 8, 256)    590080      Add_output_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_15 (Activation)      (None, 8, 8, 256)    0           conv_256_15[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_256_16 (Conv2D)            (None, 8, 8, 256)    590080      Activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_16 (Activation)      (None, 8, 8, 256)    0           conv_256_16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity256_7 (Conv2D)     (None, 8, 8, 256)    590080      Add_output_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_7 (Add)              (None, 8, 8, 256)    0           Activation_16[0][0]              \n",
      "                                                                 conv_identity256_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_256_17 (Conv2D)            (None, 8, 8, 256)    590080      Add_output_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_17 (Activation)      (None, 8, 8, 256)    0           conv_256_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_256_18 (Conv2D)            (None, 8, 8, 256)    590080      Activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_18 (Activation)      (None, 8, 8, 256)    0           conv_256_18[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity256_8 (Conv2D)     (None, 8, 8, 256)    590080      Add_output_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_8 (Add)              (None, 8, 8, 256)    0           Activation_18[0][0]              \n",
      "                                                                 conv_identity256_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool_3 (AveragePooling2D)   (None, 4, 4, 256)    0           Add_output_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_512_19 (Conv2D)            (None, 4, 4, 512)    1180160     avg_pool_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Activation_19 (Activation)      (None, 4, 4, 512)    0           conv_512_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_512_20 (Conv2D)            (None, 4, 4, 512)    2359808     Activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_20 (Activation)      (None, 4, 4, 512)    0           conv_512_20[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity512_9 (Conv2D)     (None, 4, 4, 512)    1180160     avg_pool_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_9 (Add)              (None, 4, 4, 512)    0           Activation_20[0][0]              \n",
      "                                                                 conv_identity512_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_512_21 (Conv2D)            (None, 4, 4, 512)    2359808     Add_output_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Activation_21 (Activation)      (None, 4, 4, 512)    0           conv_512_21[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_512_22 (Conv2D)            (None, 4, 4, 512)    2359808     Activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_22 (Activation)      (None, 4, 4, 512)    0           conv_512_22[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity512_10 (Conv2D)    (None, 4, 4, 512)    2359808     Add_output_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_10 (Add)             (None, 4, 4, 512)    0           Activation_22[0][0]              \n",
      "                                                                 conv_identity512_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_512_23 (Conv2D)            (None, 4, 4, 512)    2359808     Add_output_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_23 (Activation)      (None, 4, 4, 512)    0           conv_512_23[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_512_24 (Conv2D)            (None, 4, 4, 512)    2359808     Activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_24 (Activation)      (None, 4, 4, 512)    0           conv_512_24[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity512_11 (Conv2D)    (None, 4, 4, 512)    2359808     Add_output_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_11 (Add)             (None, 4, 4, 512)    0           Activation_24[0][0]              \n",
      "                                                                 conv_identity512_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_512_25 (Conv2D)            (None, 4, 4, 512)    2359808     Add_output_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_25 (Activation)      (None, 4, 4, 512)    0           conv_512_25[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_512_26 (Conv2D)            (None, 4, 4, 512)    2359808     Activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_26 (Activation)      (None, 4, 4, 512)    0           conv_512_26[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity512_12 (Conv2D)    (None, 4, 4, 512)    2359808     Add_output_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_12 (Add)             (None, 4, 4, 512)    0           Activation_26[0][0]              \n",
      "                                                                 conv_identity512_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool_4 (AveragePooling2D)   (None, 2, 2, 512)    0           Add_output_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_1024_27 (Conv2D)           (None, 2, 2, 1024)   4719616     avg_pool_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Activation_27 (Activation)      (None, 2, 2, 1024)   0           conv_1024_27[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_1024_28 (Conv2D)           (None, 2, 2, 1024)   9438208     Activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_28 (Activation)      (None, 2, 2, 1024)   0           conv_1024_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity1024_13 (Conv2D)   (None, 2, 2, 1024)   4719616     avg_pool_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_13 (Add)             (None, 2, 2, 1024)   0           Activation_28[0][0]              \n",
      "                                                                 conv_identity1024_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_1024_29 (Conv2D)           (None, 2, 2, 1024)   9438208     Add_output_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_29 (Activation)      (None, 2, 2, 1024)   0           conv_1024_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_1024_30 (Conv2D)           (None, 2, 2, 1024)   9438208     Activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_30 (Activation)      (None, 2, 2, 1024)   0           conv_1024_30[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity1024_14 (Conv2D)   (None, 2, 2, 1024)   9438208     Add_output_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_14 (Add)             (None, 2, 2, 1024)   0           Activation_30[0][0]              \n",
      "                                                                 conv_identity1024_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_1024_31 (Conv2D)           (None, 2, 2, 1024)   9438208     Add_output_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_31 (Activation)      (None, 2, 2, 1024)   0           conv_1024_31[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_1024_32 (Conv2D)           (None, 2, 2, 1024)   9438208     Activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Activation_32 (Activation)      (None, 2, 2, 1024)   0           conv_1024_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_identity1024_15 (Conv2D)   (None, 2, 2, 1024)   9438208     Add_output_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Add_output_15 (Add)             (None, 2, 2, 1024)   0           Activation_32[0][0]              \n",
      "                                                                 conv_identity1024_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           Add_output_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mlp_end (Dense)                 (None, 128)          524416      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense_output (Dense)            (None, 32094)        4140126     mlp_end[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 114,364,574\n",
      "Trainable params: 114,364,574\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800000 validated image filenames.\n",
      "Found 200000 validated image filenames.\n",
      "Epoch 1/50\n",
      "2500/2500 [==============================] - 204147s 82s/step - loss: 16.1180 - accuracy: 6.5000e-05 - val_loss: 16.1209 - val_accuracy: 5.0000e-05\n",
      "Epoch 2/50\n",
      "  78/2500 [..............................] - ETA: 54:14:23 - loss: 16.1210 - accuracy: 4.0064e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8221b2b3116d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mtraining_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyTraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_indiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#training_1.all_indiv()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtraining_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-4e5d3f080cbe>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, main_directory, current_time)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindiv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindiv_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"indiv \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindiv_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mindiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec_indiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-----------------------------------------------------------------\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         '''\n",
      "\u001b[1;32m<ipython-input-8-a7594b8d73c9>\u001b[0m in \u001b[0;36mexec_indiv\u001b[1;34m(self, main_directory, current_time)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_before_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# Sauvegarder le modèle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-a7594b8d73c9>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, train, verif, model)\u001b[0m\n\u001b[0;32m    203\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                     use_multiprocessing=False)\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[0;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[0;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m           \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[0;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m           data_format=data_format),\n\u001b[0m\u001b[0;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf2gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m         \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Création des individus (des neurals nets, ici convnet)\n",
    "list_indiv = []\n",
    "for num in range(len(list_indiv_id)):\n",
    "    list_indiv.append(IndividuResnetConv2d(\n",
    "        list_indiv_id[num],\n",
    "          list_epochs[num],\n",
    "          list_batch_size[num],\n",
    "          list_blocks_size[num],\n",
    "          list_blocks_nb[num],\n",
    "          list_l1[num],\n",
    "          list_l2[num],\n",
    "          list_batch_norm[num],\n",
    "          list_dropout[num],\n",
    "          list_filters_per_layers[num],\n",
    "          list_filters_double[num],\n",
    "          list_filters_pool[num],\n",
    "          list_activation[num],\n",
    "          list_kernel[num],\n",
    "          list_first_kernel[num],\n",
    "          list_padding[num],\n",
    "          list_max_or_avg_pool[num],\n",
    "          list_learning_r[num],\n",
    "          list_momentum[num],\n",
    "          list_optimizer[num]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Chargement de la classe training, affichag\n",
    "training_1 = MyTraining(1, list_indiv)\n",
    "#training_1.all_indiv()\n",
    "training_1.train(main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxpbXOw1RuwM"
   },
   "source": [
    "### Partie tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm6vpk3MRuwQ"
   },
   "outputs": [],
   "source": [
    "# Procedure pour utiliser tensorboard\n",
    "#  1 load la première cell\n",
    "#  2 utiliser la derniere cell avec --logdir (précisez bien votre répertoire, plus sur que ça\n",
    "#    fonctionne avec une string \"mon_path\"\n",
    "#  3 Vous NE POURREZ PLUS update tensorboard sur ce port et il y aura des bugs, pour éviter ça\n",
    "#    quand vous voulez faire une update, fermez jupyter notebook (shutdown total) et réouvrez le \n",
    "#    OU, faites kernel->interrupt et changez de port + de folder de log\n",
    "\n",
    "#si vous voulez tenter de tuer des process\n",
    "#os.system(\"taskkill /im tensorboard.exe /f\") #kill tous les processus qui utilisent tensorboard\n",
    "#os.system('!kill 18776') #kill le processus X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lj738x3Ruwa",
    "outputId": "b7c6a334-5e14-4d18-fc6e-7427bf231326"
   },
   "outputs": [],
   "source": [
    "# Liste des ports utilisés par tensorboard, attention ça se remplit vite et il faut kill jupyter pour clean\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seEJieCtRuwq",
    "outputId": "051fb9d1-b28c-4221-c61c-a8dfbb158362",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code pour démarrer tensorboard dans le dossier souhaité [PRECISEZ BIEN LE DOSSIER ICI]\n",
    "%tensorboard --logdir \"CONVNETS_20200119-0243\\logs_20200119-093909\\tensorboard_data\" --port=6066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upoC4cmYRuwz",
    "outputId": "c0390d77-555a-49f0-934c-b2740b5e2ba0"
   },
   "outputs": [],
   "source": [
    "# Si vous avez la folie des grandeurs\n",
    "notebook.display(port=6066, height=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLuEIeDvRuw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fichier CSV combined_recap + Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO0o33QYs6FP",
    "outputId": "0ebfe25a-2965-4220-ccf7-914ae220328f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commandes pandas utiles\n",
    "data_csv = pd.read_csv(main_directory + \"\\\\combined_recap.csv\")\n",
    "#data_csv = pd.read_csv(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv\")\n",
    "data.head()\n",
    "#meilleure accuracy, moins pire loss par ex\n",
    "#data_csv.sort_values([\"elapsed\"], axis=0, \n",
    "                 #ascending=[False], inplace=True) \n",
    "\n",
    "# Afficher uniquement certaines colonnes\n",
    "#dataX = data_csv.filter(items=['elapsed', 'label'])\n",
    "\n",
    "#récupérer uniquement où la loss est < à X et ou kernel = (3,3) par exemple\n",
    "#dataX = data_csv.loc[(data_csv['elapsed'] > 700) & (data_csv['threadName'].str.contains('Thread Group 1-2'))]\n",
    "#dataX\n",
    "\n",
    "#pd.set_option('display.max_rows', data3.shape[0]+1) #nombre de row max à afficher\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\logs_20200116-204456\\\\recap.csv\")\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGXNwkMCRuxE"
   },
   "outputs": [],
   "source": [
    "image = pyplot.imread(main_directory + \"\\\\logs_20200119-093909\\\\plot.png\")\n",
    "#image = pyplot.imread(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\logs_20200119-093909\\\\plot.png\")\n",
    "pyplot.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    batch_size = 32\n",
    "    train_model = tf.keras.models.load_model('fg_model.h5', custom_objects={\"xavier\":xavier})\n",
    "\n",
    "    test_datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                                      featurewise_std_normalization=False)\n",
    "\n",
    "    generator = test_datagen.flow_from_dataframe(\n",
    "            dataframe = df_images, #Limiting the test to the first 10,000 items\n",
    "            directory = DIR_TEST,\n",
    "            x_col = 'file_name',\n",
    "            target_size=(16, 16),\n",
    "            batch_size=batch_size,\n",
    "            class_mode=None,  # only data, no labels\n",
    "            shuffle=False)\n",
    "\n",
    "    family, genus, category = train_model.predict_generator(generator, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    sub = pd.DataFrame()\n",
    "    sub['Id'] = df_images.id\n",
    "    sub['Id'] = sub['Id'].astype('int32')\n",
    "\n",
    "    sub['Predicted'] = np.concatenate([np.argmax(category, axis=1), 23718*np.ones((len(df_images.id)-len(category)))], axis=0)\n",
    "    sub['Predicted'] = sub['Predicted'].astype('int32')\n",
    "    display(sub)\n",
    "    sub.to_csv('category_submission.csv', index=False)\n",
    "    \n",
    "    sub['Predicted'] = np.concatenate([np.argmax(family, axis=1), np.zeros((len(df_images.id)-len(family)))], axis=0)\n",
    "    sub['Predicted'] = sub['Predicted'].astype('int32')\n",
    "    display(sub)\n",
    "    sub.to_csv('family_submission.csv', index=False)\n",
    "    \n",
    "    sub['Predicted'] = np.concatenate([np.argmax(genus, axis=1), np.zeros((len(df_images.id)-len(genus)))], axis=0)\n",
    "    sub['Predicted'] = sub['Predicted'].astype('int32')\n",
    "    display(sub)\n",
    "    sub.to_csv('genus_submission.csv', index=False)\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    total = end_time - start_time\n",
    "    h = total//3600\n",
    "    m = (total%3600)//60\n",
    "    s = total%60\n",
    "    print(\"Total time spent: %i hours, %i minutes, and %i seconds\" %(h, m, s))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "test_several_models_1701_modif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
