{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7R0yO1XdBNh"
   },
   "source": [
    "### Démarrage de tensorboard et imports principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCCbrXXRuvi",
    "outputId": "7d91c098-39d8-4a89-adf8-3133ab8217de"
   },
   "outputs": [],
   "source": [
    "# Agrandir le notebook ?\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Commande à entrer dans le prompt anaconda si on veut éviter que notre jupyter crash\n",
    "#  sur un long entrainement\n",
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=100000000\n",
    "\n",
    "# Démarrage de tensorboard pour notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, \\\n",
    "Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil  \n",
    "from math import ceil, floor\n",
    "\n",
    "from tensorflow.python.framework import ops #pour tenter de reset tensorboard, sans grand succès\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF9xVI2bdzuT"
   },
   "source": [
    "### Hyper paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Real Test 115C -> variations du meilleur modèle acutellement trouvé avec différentes data augmentation\\n\\n# CONSTANTES : nb_layers=  8, epochs = 100, filters_per_layers = 64, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, \\n#  padding = same, avg_pool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0,\\n#  batchnorm, l1 et l2 à 0.0001, dropout à 0.4, MLP 128,\\n#  pool_frequency = 2, pool_frequency_change = (0,0), filters_double = 0 (jamais)\\n#\\n#3 tests\\n\\n#Meilleur modèle avec des datas aug différentes\\n#  * Petite data augmentation : variation sur la largeur et la hauteur de 0.1 et flip horizontal aléatoire\\n#  * Data augmentation un peu plus étoffée : pareil avec un zoom pouvant varier jusqu\\'à 30%\\n#  * Grosse Data Augmentation : pareil avec rotation pouvant aller jusq\\'à 45°\\n\\n\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\']\\nlist_epochs = [60, 60, 60]\\nlist_batch_size = [100, 100, 100]\\nlist_nb_layers = [8,8,8]\\nlist_l1 = [0.0001, 0.0001, 0.0001]\\nlist_l2 = [0.0001, 0.0001, 0.0001]\\nlist_batch_norm = [1, 1, 1]\\nlist_dropout = [0.4, 0.4, 0.4]\\nlist_filters_per_layers = [64, 64, 64]\\nlist_filters_double = [0, 0, 0]\\nlist_MLP_end = [128, 128, 128]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'avg\\',\\'avg\\',\\'avg\\']\\nlist_pool_frequency = [2, 2, 2]\\nlist_pool_frequency_change = [(0,0),(0,0),(0,0)]\\nlist_learning_r = [0.001,0.001,0.001]\\nlist_momentum = [0.9,0.9,0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Real Test 115C -> variations du meilleur modèle acutellement trouvé avec différentes data augmentation\n",
    "\n",
    "# CONSTANTES : nb_layers=  8, epochs = 100, filters_per_layers = 64, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, \n",
    "#  padding = same, avg_pool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0,\n",
    "#  batchnorm, l1 et l2 à 0.0001, dropout à 0.4, MLP 128,\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0), filters_double = 0 (jamais)\n",
    "#\n",
    "#3 tests\n",
    "\n",
    "#Meilleur modèle avec des datas aug différentes\n",
    "#  * Petite data augmentation : variation sur la largeur et la hauteur de 0.1 et flip horizontal aléatoire\n",
    "#  * Data augmentation un peu plus étoffée : pareil avec un zoom pouvant varier jusqu'à 30%\n",
    "#  * Grosse Data Augmentation : pareil avec rotation pouvant aller jusq'à 45°\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3']\n",
    "list_epochs = [60, 60, 60]\n",
    "list_batch_size = [100, 100, 100]\n",
    "list_nb_layers = [8,8,8]\n",
    "list_l1 = [0.0001, 0.0001, 0.0001]\n",
    "list_l2 = [0.0001, 0.0001, 0.0001]\n",
    "list_batch_norm = [1, 1, 1]\n",
    "list_dropout = [0.4, 0.4, 0.4]\n",
    "list_filters_per_layers = [64, 64, 64]\n",
    "list_filters_double = [0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128]\n",
    "list_activation = ['relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 15C FINAL -> meilleur modèle + faible data aug sur 200 epochs\n",
    "\n",
    "# CONSTANTES : epochs = 200, batch_size = 100, nb_layers = 8,\n",
    "# filters_per_layers à 64, pas de filters_double, MLP à 128,\n",
    "# activation relu, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "#\n",
    "#1 test\n",
    "\n",
    "#Meilleur modèle avec petite data augmentation : variation sur la largeur et la hauteur de 0.1 et flip horizontal aléatoire\n",
    "\n",
    "list_indiv_id = ['1']\n",
    "list_epochs = [200]\n",
    "list_batch_size = [100]\n",
    "list_nb_layers = [8]\n",
    "list_l1 = [0.0001]\n",
    "list_l2 = [0.0001]\n",
    "list_batch_norm = [1]\n",
    "list_dropout = [0.4]\n",
    "list_filters_per_layers = [64]\n",
    "list_filters_double = [0]\n",
    "list_MLP_end = [128]\n",
    "list_activation = ['relu']\n",
    "list_kernel = [(3,3)]\n",
    "list_padding = ['same']\n",
    "list_max_or_avg_pool = ['avg']\n",
    "list_pool_frequency = [2]\n",
    "list_pool_frequency_change = [(0,0)]\n",
    "list_learning_r = [0.001]\n",
    "list_momentum = [0.9]\n",
    "list_optimizer = ['Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIIDw6pHdJbK"
   },
   "source": [
    "### Fonctions pour préparer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tSU6Hl2Ruv0"
   },
   "outputs": [],
   "source": [
    "# Chargement des datasets de train et de validation + one hot encoding\n",
    "def load_dataset():\n",
    "    # Chargement des données cifar10\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode encoding sur les labels\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# Normalisation pour accroître la vitesse du modèle (en redimensionnant les pixels)\n",
    "def prep_pixels(train, test):\n",
    "    # Convertion des int en float\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # Normalisation pour avoir des nombres entre 0 et 1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # Retourner les images normalisées\n",
    "    return train_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTdXP2D-c2D5"
   },
   "source": [
    "### Classe Python pour définir les individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jP0CrTpRuv8"
   },
   "outputs": [],
   "source": [
    "# Classe pour les convnets\n",
    "class IndividuConvnets:\n",
    "    def __init__(self, indiv_id='1', epochs=10, batch_size=1, nb_layers=2, l1=0, l2=0, batch_norm=0,\n",
    "                 dropout=0, filters_per_layers=64, filters_double=6, MLP_end=0, activation='relu',\n",
    "                 kernel=(3,3), padding='same', max_or_avg_pool=0, pool_frequency=2,\n",
    "                 pool_frequency_change = (0,0), learning_r=0.01, momentum=0.9, optimizer='SGD'):\n",
    "        \n",
    "        # Initialisation de nos variables\n",
    "        self.time_fit = datetime.datetime.now()\n",
    "        self.my_reguralizer = None\n",
    "        \n",
    "        \n",
    "        self.nb_layers = nb_layers\n",
    "            \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.indiv_id = indiv_id\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.filters_per_layers = filters_per_layers\n",
    "                \n",
    "        self.filters_double = filters_double\n",
    "        \n",
    "        if MLP_end < 0:\n",
    "            self.MLP_end = 0\n",
    "        else:\n",
    "            self.MLP_end = MLP_end\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.max_or_avg_pool = max_or_avg_pool\n",
    "        self.pool_frequency = pool_frequency\n",
    "        self.pool_frequency_change = pool_frequency_change\n",
    "        self.learning_r = learning_r\n",
    "        self.momentum = momentum\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    # ToString()\n",
    "    def __str__(self):\n",
    "        ma_liste = []\n",
    "        ma_liste.append(\"indiv_id:{},\\n \".format(self.indiv_id))\n",
    "        ma_liste.append(\"epochs:{},\\n \".format(self.epochs))\n",
    "        ma_liste.append(\"batch_size:{},\\n \".format(self.batch_size))\n",
    "        ma_liste.append(\"nb_layers:{},\\n \".format(self.nb_layers))\n",
    "        ma_liste.append(\"l1:{},\\n \".format(self.l1))\n",
    "        ma_liste.append(\"l2:{},\\n \".format(self.l2))\n",
    "        ma_liste.append(\"batch_norm:{},\\n \".format(self.batch_norm))\n",
    "        ma_liste.append(\"dropout:{},\\n \".format(self.dropout))\n",
    "        ma_liste.append(\"filters_per_layers:{},\\n \".format(self.filters_per_layers))\n",
    "        ma_liste.append(\"filters_double:{},\\n \".format(self.filters_double))\n",
    "        ma_liste.append(\"MLP_end:{},\\n \".format(self.MLP_end))\n",
    "        ma_liste.append(\"activation:{},\\n \".format(self.activation))\n",
    "        ma_liste.append(\"kernel:\\n \")\n",
    "        ma_liste.append(\"{},\\n \".format(self.kernel))\n",
    "        ma_liste.append(\"padding:{},\\n \".format(self.padding))\n",
    "        ma_liste.append(\"max_or_avg_pool:{}\\n\".format(self.max_or_avg_pool))\n",
    "        ma_liste.append(\"pool_frequency:{}\\n\".format(self.pool_frequency))\n",
    "        ma_liste.append(\"pool_frequency_change:{}\\n\".format(self.pool_frequency_change))\n",
    "        ma_liste.append(\"learning_r:{}\\n\".format(self.learning_r))\n",
    "        ma_liste.append(\"momentum:{}\\n\".format(self.momentum))\n",
    "        ma_liste.append(\"optimizer:{}\\n\".format(self.optimizer))\n",
    "            \n",
    "        return ma_liste\n",
    "    \n",
    "    # (Modele 2 conv + norm ? + pool) * X -> MLP -> softmax sortie 10 -> MODELE BLOC 2\n",
    "    # D'autres modeles seront crees par la suite\n",
    "    def create_and_train_model(self, trainX, trainY, testX, testY, main_directory):\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        # Choix d'un emplacement pour les logs\n",
    "        log_dir=main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\tensorboard_data\\\\\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        print(\"log dir = \",log_dir)\n",
    "        \n",
    "        # l1 et l2\n",
    "        if self.l1 > 0 and self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l1_l2(l1=self.l1 / self.nb_layers,\n",
    "                                    l2=self.l2 / self.nb_layers)\n",
    "        elif self.l1 > 0:\n",
    "            self.my_regularizer = regularizers.l1(self.l1 / self.nb_layers)\n",
    "        elif self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l2(self.l2 / self.nb_layers)\n",
    "        else:\n",
    "            self.my_reguralizer = None\n",
    "            \n",
    "        # Definir notre modèle basique\n",
    "        model = Sequential()\n",
    "\n",
    "        # Faire toutes les convs nécessaires\n",
    "        counter_filters_double = 0 # Var pour doubler les filtres\n",
    "        counter_pool_freq = 0 # var pour savoir où placer les couches de pooling\n",
    "        counter_pool = 0 # var pour nommer les max / avg pool\n",
    "        \n",
    "        #initialisation de variables locales pour ne pas modifier nos attributs evolutifs\n",
    "        # qui doivent être log à la fin\n",
    "        pool_frequency = self.pool_frequency\n",
    "        filters_per_layers = self.filters_per_layers\n",
    "        \n",
    "        for i in range(0, self.nb_layers):\n",
    "            \n",
    "            print(\"counter_pool_freq = \", counter_pool_freq)\n",
    "            # Traitement pour doubler les filtres tous les X couches de convo\n",
    "            if counter_filters_double >= self.filters_double and self.filters_double > 0:\n",
    "                filters_per_layers = filters_per_layers * 2\n",
    "                print(\"filters = \", filters_per_layers)\n",
    "                counter_filters_double = 0\n",
    "            \n",
    "            # Première conv, on fixe l'input shape\n",
    "            if i == 0:\n",
    "                model.add(Conv2D(filters_per_layers, self.kernel, activation=self.activation,\n",
    "                    kernel_regularizer=self.my_reguralizer, padding=self.padding,\n",
    "                    input_shape=(32, 32, 3), name='conv_'+str(filters_per_layers)+'_'+str(i+1)))\n",
    "            else:\n",
    "                # Couche de conv + rajouts selon nos hyperparams\n",
    "                model.add(Conv2D(filters_per_layers, self.kernel, activation=self.activation,\n",
    "                    kernel_regularizer=self.my_reguralizer, padding=self.padding,\n",
    "                                 name='conv_'+str(filters_per_layers)+'_'+str(i+1)))\n",
    "            \n",
    "            # Après avoir créé une conv on incrémente nos compteurs (sauf counter_pool)\n",
    "            counter_filters_double = counter_filters_double + 1\n",
    "            counter_pool_freq = counter_pool_freq + 1\n",
    "            \n",
    "            # Ajouts de la regularization / du pooling selon les hyperparamètres saisis\n",
    "            \n",
    "            if self.batch_norm == 1:\n",
    "                model.add(BatchNormalization( name='batchnorm_'+str(i+1)))\n",
    "            \n",
    "            if pool_frequency == counter_pool_freq:    \n",
    "                #go max ou avg pooling\n",
    "                if self.max_or_avg_pool == 'max':\n",
    "                    model.add(MaxPooling2D((2, 2), padding='same', \n",
    "                        name='max_pool_'+str(counter_pool+1)))\n",
    "                    counter_pool = counter_pool + 1\n",
    "                else:\n",
    "                    model.add(AveragePooling2D((2, 2), padding='same', \n",
    "                                name='avg_pool_'+str(counter_pool+1)))\n",
    "                    counter_pool = counter_pool + 1\n",
    "                \n",
    "                # Dropout sur les pools \n",
    "                if self.dropout > 0:\n",
    "                    model.add(Dropout(self.dropout, \n",
    "                                      name='Dropout_'+str(self.dropout)+'_'+str(counter_pool+1)))\n",
    "                    \n",
    "                # si filters_double à -1, on double les filtres apres le pooling\n",
    "                if self.filters_double == -1:\n",
    "                    filters_per_layers = filters_per_layers * 2\n",
    "                \n",
    "                # Après avoir mis un pool, on regarde si l'on doit changer ou non \n",
    "                #  la freq d'apparition de pools\n",
    "                if counter_pool == self.pool_frequency_change[0] \\\n",
    "                    and self.pool_frequency_change[0] != 0:\n",
    "                    \n",
    "                    pool_frequency = pool_frequency + self.pool_frequency_change[1]\n",
    "                    \n",
    "                counter_pool_freq = 0\n",
    "            \n",
    "        \n",
    "        # Fin des convs -> neural network classique\n",
    "        model.add(Flatten(name='Flatten'))\n",
    "        \n",
    "        print(\"i after for = \", i)\n",
    "        #tTrain dans un MLP avant la fin si on le souhaite\n",
    "        if self.MLP_end > 0:\n",
    "            model.add(Dense(128, activation='relu', kernel_regularizer=self.my_reguralizer,\n",
    "                            name='MLP_'+str(self.MLP_end)))\n",
    "            if self.batch_norm == 1:\n",
    "                model.add(BatchNormalization( name='batchnorm_finale'))\n",
    "            \n",
    "            #mettre dropout sur les Dense, moins opti sur des pools ? à tester si tmp ok\n",
    "            #(pas conv car importantes)\n",
    "            if self.dropout > 0:\n",
    "                model.add(Dropout(self.dropout, name='Dropout_'+str(self.dropout)+'_final'))\n",
    "        \n",
    "        #notre output\n",
    "        model.add(Dense(10, activation='softmax', name='output')) \n",
    "\n",
    "        # Compiler le modele\n",
    "        if self.optimizer == 'SGD':\n",
    "            print(\"SGD, learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = SGD(lr=self.learning_r, momentum=self.momentum)\n",
    "        else:\n",
    "            print(\"Adam learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = Adam(lr=self.learning_r, beta_1=self.momentum) # beta_1 => notation momentum Adam\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # APPLICATION DE LA DATA AUGMENTATION\n",
    "        # On crée un générateur d'images (avec peu, pas mal ou beaucoup de variations)\n",
    "        if self.indiv_id == '1':\n",
    "            datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "            print(\"Simple data aug\")\n",
    "        elif self.indiv_id == '2':\n",
    "            datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, zoom_range=0.3)\n",
    "            print(\"Moyenne data aug\")\n",
    "        else:\n",
    "            datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, zoom_range=0.3, rotation_range=45)\n",
    "            print(\"Grosse data aug\")\n",
    "        # On crée un \"iterator\"\n",
    "        it_train = datagen.flow(trainX, trainY, batch_size=self.batch_size)\n",
    "        #On va devoir ajouter l'attribut steps_per_epoch au fit, il donne le nombre d'expemples de data aug présents dans chaque epoch\n",
    "        steps = int(trainX.shape[0] / self.batch_size)\n",
    "        \n",
    "        # Entrainer le modele\n",
    "        history = model.fit_generator(it_train, steps_per_epoch=steps, epochs=self.epochs,\n",
    "                        validation_data=(testX, testY), verbose=1, callbacks=[tensorboard_callback])\n",
    "        \n",
    "        # Garder une trace du temps nécessaire pour fit (peut être pas la meilleure méthode)\n",
    "        end = datetime.datetime.now()\n",
    "        self.time_fit = end - start\n",
    "        print(\"\\nTime for fit = \", round(self.time_fit.total_seconds(),2)) # Round total_seconds()\n",
    "\n",
    "        return history, model\n",
    "    \n",
    "    \n",
    "    def save_model(self, history, model, main_directory, current_time):\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        plot_model(model, \"model.png\")\n",
    "        \n",
    "        # Deplacement modele au bon endroit\n",
    "        shutil.move(os.getcwd()+\"\\\\model.png\", main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\model.png\")\n",
    "        \n",
    "        # Afficher nos résultats dans un graphique matplotlib sauvegardé\n",
    "        pyplot.gcf().subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # Afficher la loss\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "        \n",
    "        # Afficher l'accuracy\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "        \n",
    "        # Sauvegarde\n",
    "        filename = main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\\"\n",
    "        pyplot.savefig(filename + 'plot.png')\n",
    "        pyplot.close()\n",
    "       \n",
    "        \n",
    "        print(\"LOSS : \", round(history.history['loss'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_LOSS : \", round(history.history['val_loss'][self.epochs-1].item(), 3))\n",
    "        print(\"ACCURACY : \", round(history.history['accuracy'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_ACCURACY : \", round(history.history['val_accuracy'][self.epochs-1].item(), 3))\n",
    "        \n",
    "        # attributs pour créer les csv indivudels et le csv global\n",
    "        self.loss = round(history.history['loss'][self.epochs-1].item(), 3)\n",
    "        self.val_loss = round(history.history['val_loss'][self.epochs-1].item(), 3)\n",
    "        self.accuracy = round(history.history['accuracy'][self.epochs-1].item(), 3)\n",
    "        self.val_accuracy = round(history.history['val_accuracy'][self.epochs-1].item(), 3)\n",
    "        self.time_taken = round(self.time_fit.total_seconds(),2)\n",
    "        \n",
    "        # Créer un dataframe pandas (avec hyperparams) et le sauvegarder en CSV\n",
    "        df = pd.DataFrame({'indiv_id': [self.indiv_id],\n",
    "                           'epochs': [self.epochs],\n",
    "                           'batch_size': [self.batch_size],\n",
    "                           'nb_layers': [self.nb_layers],\n",
    "                           'l1': [self.l1],\n",
    "                           'l2': [self.l2],\n",
    "                           'batch_norm': [self.batch_norm],\n",
    "                           'dropout': [self.dropout],\n",
    "                           'filters_per_layers': [self.filters_per_layers],\n",
    "                           'filters_double': [self.filters_double],\n",
    "                           'MLP_end': [self.MLP_end],\n",
    "                           'activation': [self.activation],\n",
    "                           'kernel': [self.kernel],\n",
    "                           'padding': [self.padding],\n",
    "                           'max_or_avg_pool': [self.max_or_avg_pool],\n",
    "                           'pool_frequency': [self.pool_frequency],\n",
    "                           'pool_frequency_change': [self.pool_frequency_change],\n",
    "                           'loss': [self.loss],\n",
    "                           'val_loss': [self.val_loss],\n",
    "                           'accuracy': [self.accuracy],\n",
    "                           'val_accuracy': [self.val_accuracy],\n",
    "                           'time_taken' : [self.time_taken],\n",
    "                           'learning_r' : [self.learning_r],\n",
    "                           'momentum' : [self.momentum],\n",
    "                           'optimizer' : [self.optimizer]\n",
    "                          })\n",
    "        \n",
    "        df.to_csv(path_or_buf=filename+\"recap.csv\",index=False)\n",
    "    \n",
    "    # Lance toutes les étapes\n",
    "    def exec_indiv(self, main_directory, current_time):\n",
    "        \n",
    "        # Charger les données\n",
    "        trainX, trainY, testX, testY = load_dataset()\n",
    "        \n",
    "        # Normaliser les données\n",
    "        trainX, testX = prep_pixels(trainX, testX)\n",
    "        \n",
    "        print(\"TrainX shape = \",np.shape(trainX))\n",
    "        print(\"TestX shape = \",np.shape(testX), \"\\n\")\n",
    "        # Créer et entrainer le modele\n",
    "        history, model = self.create_and_train_model(trainX, trainY, testX, testY, main_directory)\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        save = self.save_model(history, model, main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buU7Dsqfs6Cv"
   },
   "source": [
    "### Classe Python qui va démarrer les tests des neural nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im542kkes6Cx"
   },
   "outputs": [],
   "source": [
    "# Classe générale qui va nous servir à effectuer des actions sur des individus\n",
    "class MyTraining:\n",
    "    # Prends un ID et une liste d'individus \n",
    "    def __init__(self, id_train, indiv_list):\n",
    "        \n",
    "        self.id_train = id_train\n",
    "        self.indiv_list = indiv_list\n",
    "    \n",
    "    def train(self, main_directory, current_time):\n",
    "        \n",
    "        print(\"Start training\\n\")\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            print(\"indiv \", indiv.indiv_id, \"\\n\")\n",
    "            indiv.exec_indiv(main_directory, current_time)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "        \n",
    "        # Fusion des csv \n",
    "        merge_csv = pd.DataFrame(columns=['indiv_id', 'epochs', 'nb_layers', 'l1', 'l2', 'batch_norm', 'dropout',\n",
    "                                          'filters_per_layers', 'filters_double', 'MLP_end', 'activation', 'kernel',\n",
    "                                          'padding','max_or_avg_pool', 'pool_frequency', 'pool_frequency_change', 'loss',\n",
    "                                          'val_loss', 'accuracy', 'val_accuracy', 'time_taken','learning_r',\n",
    "                                          'momentum', 'optimizer'])\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            merge_csv = merge_csv.append(\n",
    "                             {'indiv_id': indiv.indiv_id, 'epochs': indiv.epochs, 'batch_size': indiv.batch_size,\n",
    "                              'nb_layers' : indiv.nb_layers,'l1' : indiv.l1, 'l2' : indiv.l2, 'batch_norm': indiv.batch_norm,\n",
    "                              'dropout' : indiv.dropout,'filters_per_layers' : indiv.filters_per_layers,\n",
    "                              'filters_double' : indiv.filters_double,'MLP_end' : indiv.MLP_end,\n",
    "                              'activation' : indiv.activation,'kernel' : indiv.kernel,'padding' : indiv.padding,\n",
    "                              'max_or_avg_pool' : indiv.max_or_avg_pool,'pool_frequency' : indiv.pool_frequency,\n",
    "                              'pool_frequency_change' : indiv.pool_frequency_change,'loss' : indiv.loss,\n",
    "                              'val_loss' : indiv.val_loss,'accuracy' : indiv.accuracy, 'val_accuracy' : indiv.val_accuracy,\n",
    "                              'time_taken' : indiv.time_taken,'learning_r' : indiv.learning_r,'momentum': indiv.momentum,\n",
    "                              'optimizer' : indiv.optimizer\n",
    "                             },ignore_index=True)\n",
    "        \n",
    "        # sauvegarde\n",
    "        merge_csv.to_csv(main_directory+\"\\\\combined_recap.csv\", index=False)\n",
    "            \n",
    "    \n",
    "    def all_indiv(self):\n",
    "        \n",
    "        # Affiche les caractéristiques de l'ensemble des individus\n",
    "        for indiv in self.indiv_list:\n",
    "            print('\\n'.join(indiv.__str__()))\n",
    "            for tir in range(80): print('-', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIGBnz_4bj74"
   },
   "source": [
    "### Traitement général (train de l'ensemble des modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjIV_jF6RuwF",
    "outputId": "6759a6e1-3664-4264-c87d-18266651af09",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indiv_id:1,\n",
      " \n",
      "epochs:200,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0.0001,\n",
      " \n",
      "l2:0.0001,\n",
      " \n",
      "batch_norm:1,\n",
      " \n",
      "dropout:0.4,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:avg\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start training\n",
      "\n",
      "indiv  1 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-10-00-18\\log_1\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "i after for =  7\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Simple data aug\n",
      "Epoch 1/200\n",
      "500/500 [==============================] - 81s 162ms/step - loss: 1.9071 - accuracy: 0.3348 - val_loss: 1.8948 - val_accuracy: 0.3297\n",
      "Epoch 2/200\n",
      "500/500 [==============================] - 103s 207ms/step - loss: 1.3990 - accuracy: 0.4954 - val_loss: 1.4331 - val_accuracy: 0.5195\n",
      "Epoch 3/200\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 1.1613 - accuracy: 0.5860 - val_loss: 0.9679 - val_accuracy: 0.6637\n",
      "Epoch 4/200\n",
      "500/500 [==============================] - 129s 259ms/step - loss: 1.0151 - accuracy: 0.6418 - val_loss: 0.9732 - val_accuracy: 0.6623\n",
      "Epoch 5/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.9242 - accuracy: 0.6792 - val_loss: 1.0177 - val_accuracy: 0.6720\n",
      "Epoch 6/200\n",
      "500/500 [==============================] - 134s 268ms/step - loss: 0.8597 - accuracy: 0.7067 - val_loss: 0.8872 - val_accuracy: 0.7008\n",
      "Epoch 7/200\n",
      "500/500 [==============================] - 142s 284ms/step - loss: 0.8118 - accuracy: 0.7243 - val_loss: 0.8559 - val_accuracy: 0.7156\n",
      "Epoch 8/200\n",
      "500/500 [==============================] - 142s 284ms/step - loss: 0.7726 - accuracy: 0.7386 - val_loss: 0.7744 - val_accuracy: 0.7521\n",
      "Epoch 9/200\n",
      "500/500 [==============================] - 153s 307ms/step - loss: 0.7420 - accuracy: 0.7473 - val_loss: 0.7755 - val_accuracy: 0.7417\n",
      "Epoch 10/200\n",
      "500/500 [==============================] - 140s 280ms/step - loss: 0.7187 - accuracy: 0.7583 - val_loss: 0.6648 - val_accuracy: 0.7751\n",
      "Epoch 11/200\n",
      "500/500 [==============================] - 142s 284ms/step - loss: 0.6902 - accuracy: 0.7664 - val_loss: 0.7060 - val_accuracy: 0.7739\n",
      "Epoch 12/200\n",
      "500/500 [==============================] - 164s 329ms/step - loss: 0.6703 - accuracy: 0.7738 - val_loss: 0.8945 - val_accuracy: 0.7267\n",
      "Epoch 13/200\n",
      "500/500 [==============================] - 147s 294ms/step - loss: 0.6515 - accuracy: 0.7797 - val_loss: 0.8601 - val_accuracy: 0.7251\n",
      "Epoch 14/200\n",
      "500/500 [==============================] - 162s 324ms/step - loss: 0.6398 - accuracy: 0.7847 - val_loss: 0.6254 - val_accuracy: 0.7931\n",
      "Epoch 15/200\n",
      "500/500 [==============================] - 161s 321ms/step - loss: 0.6242 - accuracy: 0.7885 - val_loss: 0.6339 - val_accuracy: 0.7906\n",
      "Epoch 16/200\n",
      "500/500 [==============================] - 163s 326ms/step - loss: 0.6063 - accuracy: 0.7976 - val_loss: 0.5952 - val_accuracy: 0.8097\n",
      "Epoch 17/200\n",
      "500/500 [==============================] - 162s 324ms/step - loss: 0.5973 - accuracy: 0.7976 - val_loss: 0.6056 - val_accuracy: 0.7980\n",
      "Epoch 18/200\n",
      "500/500 [==============================] - 143s 285ms/step - loss: 0.5846 - accuracy: 0.8029 - val_loss: 0.6863 - val_accuracy: 0.7719\n",
      "Epoch 19/200\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 0.5722 - accuracy: 0.8084 - val_loss: 0.7412 - val_accuracy: 0.7635\n",
      "Epoch 20/200\n",
      "500/500 [==============================] - 170s 341ms/step - loss: 0.5642 - accuracy: 0.8087 - val_loss: 0.5193 - val_accuracy: 0.8315\n",
      "Epoch 21/200\n",
      "500/500 [==============================] - 1332s 3s/step - loss: 0.5483 - accuracy: 0.8156 - val_loss: 0.5395 - val_accuracy: 0.8198\n",
      "Epoch 22/200\n",
      "500/500 [==============================] - 322s 643ms/step - loss: 0.5515 - accuracy: 0.8150 - val_loss: 0.5127 - val_accuracy: 0.8290\n",
      "Epoch 23/200\n",
      "500/500 [==============================] - 245s 490ms/step - loss: 0.5422 - accuracy: 0.8177 - val_loss: 0.5365 - val_accuracy: 0.8238\n",
      "Epoch 24/200\n",
      "500/500 [==============================] - 252s 505ms/step - loss: 0.5328 - accuracy: 0.8197 - val_loss: 0.5232 - val_accuracy: 0.8261\n",
      "Epoch 25/200\n",
      "500/500 [==============================] - 336s 672ms/step - loss: 0.5244 - accuracy: 0.8236 - val_loss: 0.6042 - val_accuracy: 0.8038\n",
      "Epoch 26/200\n",
      "500/500 [==============================] - 471s 941ms/step - loss: 0.5216 - accuracy: 0.8249 - val_loss: 0.5184 - val_accuracy: 0.8245\n",
      "Epoch 27/200\n",
      "500/500 [==============================] - 189s 377ms/step - loss: 0.5162 - accuracy: 0.8258 - val_loss: 0.5291 - val_accuracy: 0.8282\n",
      "Epoch 28/200\n",
      "500/500 [==============================] - 177s 354ms/step - loss: 0.5081 - accuracy: 0.8307 - val_loss: 0.4853 - val_accuracy: 0.8400\n",
      "Epoch 29/200\n",
      "500/500 [==============================] - 179s 358ms/step - loss: 0.5020 - accuracy: 0.8311 - val_loss: 0.5626 - val_accuracy: 0.8208\n",
      "Epoch 30/200\n",
      "500/500 [==============================] - 175s 350ms/step - loss: 0.4978 - accuracy: 0.8342 - val_loss: 0.4228 - val_accuracy: 0.8590\n",
      "Epoch 31/200\n",
      "500/500 [==============================] - 168s 336ms/step - loss: 0.4943 - accuracy: 0.8350 - val_loss: 0.5630 - val_accuracy: 0.8180\n",
      "Epoch 32/200\n",
      "500/500 [==============================] - 167s 333ms/step - loss: 0.4879 - accuracy: 0.8362 - val_loss: 0.4909 - val_accuracy: 0.8358\n",
      "Epoch 33/200\n",
      "500/500 [==============================] - 164s 328ms/step - loss: 0.4828 - accuracy: 0.8381 - val_loss: 0.4312 - val_accuracy: 0.8584\n",
      "Epoch 34/200\n",
      "500/500 [==============================] - 170s 339ms/step - loss: 0.4804 - accuracy: 0.8384 - val_loss: 0.5701 - val_accuracy: 0.8202\n",
      "Epoch 35/200\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.4762 - accuracy: 0.8413 - val_loss: 0.4717 - val_accuracy: 0.8447\n",
      "Epoch 36/200\n",
      "500/500 [==============================] - 168s 336ms/step - loss: 0.4719 - accuracy: 0.8413 - val_loss: 0.4956 - val_accuracy: 0.8397\n",
      "Epoch 37/200\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.4665 - accuracy: 0.8431 - val_loss: 0.5241 - val_accuracy: 0.8268\n",
      "Epoch 38/200\n",
      "500/500 [==============================] - 166s 332ms/step - loss: 0.4656 - accuracy: 0.8438 - val_loss: 0.4583 - val_accuracy: 0.8447\n",
      "Epoch 39/200\n",
      "500/500 [==============================] - 159s 318ms/step - loss: 0.4565 - accuracy: 0.8468 - val_loss: 0.4527 - val_accuracy: 0.8500\n",
      "Epoch 40/200\n",
      "500/500 [==============================] - 161s 322ms/step - loss: 0.4591 - accuracy: 0.8455 - val_loss: 0.4413 - val_accuracy: 0.8516\n",
      "Epoch 41/200\n",
      "500/500 [==============================] - 160s 319ms/step - loss: 0.4618 - accuracy: 0.8457 - val_loss: 0.4694 - val_accuracy: 0.8465\n",
      "Epoch 42/200\n",
      "500/500 [==============================] - 161s 321ms/step - loss: 0.4566 - accuracy: 0.8477 - val_loss: 0.7583 - val_accuracy: 0.7716\n",
      "Epoch 43/200\n",
      "500/500 [==============================] - 157s 315ms/step - loss: 0.4514 - accuracy: 0.8478 - val_loss: 0.6170 - val_accuracy: 0.8049\n",
      "Epoch 44/200\n",
      "500/500 [==============================] - 158s 316ms/step - loss: 0.4502 - accuracy: 0.8492 - val_loss: 0.4536 - val_accuracy: 0.8520\n",
      "Epoch 45/200\n",
      "500/500 [==============================] - 154s 308ms/step - loss: 0.4495 - accuracy: 0.8494 - val_loss: 0.4587 - val_accuracy: 0.8507\n",
      "Epoch 46/200\n",
      "500/500 [==============================] - 155s 310ms/step - loss: 0.4452 - accuracy: 0.8485 - val_loss: 0.5041 - val_accuracy: 0.8369\n",
      "Epoch 47/200\n",
      "500/500 [==============================] - 153s 306ms/step - loss: 0.4393 - accuracy: 0.8528 - val_loss: 0.4649 - val_accuracy: 0.8454\n",
      "Epoch 48/200\n",
      "500/500 [==============================] - 153s 305ms/step - loss: 0.4388 - accuracy: 0.8532 - val_loss: 0.5667 - val_accuracy: 0.8152\n",
      "Epoch 49/200\n",
      "500/500 [==============================] - 159s 318ms/step - loss: 0.4342 - accuracy: 0.8562 - val_loss: 0.4755 - val_accuracy: 0.8454\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 153s 305ms/step - loss: 0.4333 - accuracy: 0.8550 - val_loss: 0.5061 - val_accuracy: 0.8410\n",
      "Epoch 51/200\n",
      "500/500 [==============================] - 151s 302ms/step - loss: 0.4266 - accuracy: 0.8575 - val_loss: 0.4274 - val_accuracy: 0.8556\n",
      "Epoch 52/200\n",
      "500/500 [==============================] - 154s 308ms/step - loss: 0.4269 - accuracy: 0.8556 - val_loss: 0.5610 - val_accuracy: 0.8256\n",
      "Epoch 53/200\n",
      "500/500 [==============================] - 149s 297ms/step - loss: 0.4265 - accuracy: 0.8571 - val_loss: 0.4405 - val_accuracy: 0.8539\n",
      "Epoch 54/200\n",
      "500/500 [==============================] - 152s 304ms/step - loss: 0.4218 - accuracy: 0.8595 - val_loss: 0.4851 - val_accuracy: 0.8435\n",
      "Epoch 55/200\n",
      "500/500 [==============================] - 170s 340ms/step - loss: 0.4178 - accuracy: 0.8594 - val_loss: 0.4959 - val_accuracy: 0.8403\n",
      "Epoch 56/200\n",
      "500/500 [==============================] - 183s 365ms/step - loss: 0.4168 - accuracy: 0.8605 - val_loss: 0.4861 - val_accuracy: 0.8476\n",
      "Epoch 57/200\n",
      "500/500 [==============================] - 165s 329ms/step - loss: 0.4193 - accuracy: 0.8597 - val_loss: 0.4581 - val_accuracy: 0.8526\n",
      "Epoch 58/200\n",
      "500/500 [==============================] - 151s 302ms/step - loss: 0.4126 - accuracy: 0.8628 - val_loss: 0.4945 - val_accuracy: 0.8420\n",
      "Epoch 59/200\n",
      "500/500 [==============================] - 156s 312ms/step - loss: 0.4139 - accuracy: 0.8603 - val_loss: 0.4589 - val_accuracy: 0.8534\n",
      "Epoch 60/200\n",
      "500/500 [==============================] - 149s 299ms/step - loss: 0.4094 - accuracy: 0.8627 - val_loss: 0.4259 - val_accuracy: 0.8621\n",
      "Epoch 61/200\n",
      "500/500 [==============================] - 148s 295ms/step - loss: 0.4112 - accuracy: 0.8617 - val_loss: 0.4244 - val_accuracy: 0.8616\n",
      "Epoch 62/200\n",
      "500/500 [==============================] - 156s 312ms/step - loss: 0.4051 - accuracy: 0.8626 - val_loss: 0.5210 - val_accuracy: 0.8349\n",
      "Epoch 63/200\n",
      "500/500 [==============================] - 154s 308ms/step - loss: 0.4035 - accuracy: 0.8635 - val_loss: 0.4249 - val_accuracy: 0.8600\n",
      "Epoch 64/200\n",
      "500/500 [==============================] - 149s 299ms/step - loss: 0.4080 - accuracy: 0.8619 - val_loss: 0.4054 - val_accuracy: 0.8655\n",
      "Epoch 65/200\n",
      "500/500 [==============================] - 149s 298ms/step - loss: 0.4016 - accuracy: 0.8647 - val_loss: 0.5337 - val_accuracy: 0.8351\n",
      "Epoch 66/200\n",
      "500/500 [==============================] - 155s 309ms/step - loss: 0.4028 - accuracy: 0.8652 - val_loss: 0.4790 - val_accuracy: 0.8488\n",
      "Epoch 67/200\n",
      "500/500 [==============================] - 155s 309ms/step - loss: 0.3976 - accuracy: 0.8658 - val_loss: 0.3832 - val_accuracy: 0.8731\n",
      "Epoch 68/200\n",
      "500/500 [==============================] - 151s 303ms/step - loss: 0.3960 - accuracy: 0.8669 - val_loss: 0.4045 - val_accuracy: 0.8729\n",
      "Epoch 69/200\n",
      "500/500 [==============================] - 153s 307ms/step - loss: 0.3973 - accuracy: 0.8657 - val_loss: 0.4618 - val_accuracy: 0.8511\n",
      "Epoch 70/200\n",
      "500/500 [==============================] - 153s 305ms/step - loss: 0.3967 - accuracy: 0.8662 - val_loss: 0.3984 - val_accuracy: 0.8693\n",
      "Epoch 71/200\n",
      "500/500 [==============================] - 150s 300ms/step - loss: 0.3935 - accuracy: 0.8672 - val_loss: 0.3627 - val_accuracy: 0.8806\n",
      "Epoch 72/200\n",
      "500/500 [==============================] - 156s 312ms/step - loss: 0.3995 - accuracy: 0.8649 - val_loss: 0.4249 - val_accuracy: 0.8618\n",
      "Epoch 73/200\n",
      "500/500 [==============================] - 152s 304ms/step - loss: 0.3895 - accuracy: 0.8697 - val_loss: 0.4456 - val_accuracy: 0.8581\n",
      "Epoch 74/200\n",
      "500/500 [==============================] - 152s 303ms/step - loss: 0.3905 - accuracy: 0.8676 - val_loss: 0.4364 - val_accuracy: 0.8586\n",
      "Epoch 75/200\n",
      "500/500 [==============================] - 157s 314ms/step - loss: 0.3871 - accuracy: 0.8699 - val_loss: 0.3880 - val_accuracy: 0.8720\n",
      "Epoch 76/200\n",
      "500/500 [==============================] - 150s 300ms/step - loss: 0.3875 - accuracy: 0.8680 - val_loss: 0.3843 - val_accuracy: 0.8743\n",
      "Epoch 77/200\n",
      "500/500 [==============================] - 153s 305ms/step - loss: 0.3875 - accuracy: 0.8701 - val_loss: 0.4171 - val_accuracy: 0.8653\n",
      "Epoch 78/200\n",
      "500/500 [==============================] - 153s 307ms/step - loss: 0.3861 - accuracy: 0.8696 - val_loss: 0.4181 - val_accuracy: 0.8658\n",
      "Epoch 79/200\n",
      "500/500 [==============================] - 154s 307ms/step - loss: 0.3815 - accuracy: 0.8699 - val_loss: 0.3965 - val_accuracy: 0.8676\n",
      "Epoch 80/200\n",
      "500/500 [==============================] - 149s 298ms/step - loss: 0.3788 - accuracy: 0.8729 - val_loss: 0.3797 - val_accuracy: 0.8757\n",
      "Epoch 81/200\n",
      "500/500 [==============================] - 162s 324ms/step - loss: 0.3777 - accuracy: 0.8728 - val_loss: 0.3827 - val_accuracy: 0.8724\n",
      "Epoch 82/200\n",
      "500/500 [==============================] - 157s 315ms/step - loss: 0.3801 - accuracy: 0.8720 - val_loss: 0.3907 - val_accuracy: 0.8726\n",
      "Epoch 83/200\n",
      "500/500 [==============================] - 148s 296ms/step - loss: 0.3771 - accuracy: 0.8708 - val_loss: 0.3888 - val_accuracy: 0.8736\n",
      "Epoch 84/200\n",
      "500/500 [==============================] - 147s 294ms/step - loss: 0.3717 - accuracy: 0.8749 - val_loss: 0.4165 - val_accuracy: 0.8690\n",
      "Epoch 85/200\n",
      "500/500 [==============================] - 147s 295ms/step - loss: 0.3780 - accuracy: 0.8718 - val_loss: 0.4292 - val_accuracy: 0.8632\n",
      "Epoch 86/200\n",
      "500/500 [==============================] - 154s 307ms/step - loss: 0.3739 - accuracy: 0.8741 - val_loss: 0.4388 - val_accuracy: 0.8590\n",
      "Epoch 87/200\n",
      "500/500 [==============================] - 149s 298ms/step - loss: 0.3726 - accuracy: 0.8721 - val_loss: 0.3817 - val_accuracy: 0.8726\n",
      "Epoch 88/200\n",
      "500/500 [==============================] - 149s 299ms/step - loss: 0.3689 - accuracy: 0.8761 - val_loss: 0.4127 - val_accuracy: 0.8673\n",
      "Epoch 89/200\n",
      "500/500 [==============================] - 155s 310ms/step - loss: 0.3711 - accuracy: 0.8747 - val_loss: 0.4006 - val_accuracy: 0.8712\n",
      "Epoch 90/200\n",
      "500/500 [==============================] - 153s 307ms/step - loss: 0.3698 - accuracy: 0.8739 - val_loss: 0.3510 - val_accuracy: 0.8859\n",
      "Epoch 91/200\n",
      "500/500 [==============================] - 152s 303ms/step - loss: 0.3733 - accuracy: 0.8730 - val_loss: 0.3489 - val_accuracy: 0.8824\n",
      "Epoch 92/200\n",
      "500/500 [==============================] - 148s 295ms/step - loss: 0.3632 - accuracy: 0.8764 - val_loss: 0.4715 - val_accuracy: 0.8523\n",
      "Epoch 93/200\n",
      "500/500 [==============================] - 153s 306ms/step - loss: 0.3666 - accuracy: 0.8763 - val_loss: 0.3890 - val_accuracy: 0.8755\n",
      "Epoch 94/200\n",
      "500/500 [==============================] - 150s 301ms/step - loss: 0.3678 - accuracy: 0.8767 - val_loss: 0.3445 - val_accuracy: 0.8846\n",
      "Epoch 95/200\n",
      "500/500 [==============================] - 148s 297ms/step - loss: 0.3606 - accuracy: 0.8767 - val_loss: 0.3800 - val_accuracy: 0.8759\n",
      "Epoch 96/200\n",
      "500/500 [==============================] - 156s 311ms/step - loss: 0.3657 - accuracy: 0.8768 - val_loss: 0.4033 - val_accuracy: 0.8709\n",
      "Epoch 97/200\n",
      "500/500 [==============================] - 149s 298ms/step - loss: 0.3595 - accuracy: 0.8770 - val_loss: 0.3535 - val_accuracy: 0.8854\n",
      "Epoch 98/200\n",
      "500/500 [==============================] - 149s 299ms/step - loss: 0.3596 - accuracy: 0.8778 - val_loss: 0.3656 - val_accuracy: 0.8827\n",
      "Epoch 99/200\n",
      "500/500 [==============================] - 151s 303ms/step - loss: 0.3588 - accuracy: 0.8777 - val_loss: 0.4769 - val_accuracy: 0.8494\n",
      "Epoch 100/200\n",
      "500/500 [==============================] - 152s 304ms/step - loss: 0.3588 - accuracy: 0.8774 - val_loss: 0.3866 - val_accuracy: 0.8714\n",
      "Epoch 101/200\n",
      "500/500 [==============================] - 147s 294ms/step - loss: 0.3598 - accuracy: 0.8777 - val_loss: 0.3863 - val_accuracy: 0.8723\n",
      "Epoch 102/200\n",
      "500/500 [==============================] - 149s 298ms/step - loss: 0.3592 - accuracy: 0.8791 - val_loss: 0.4298 - val_accuracy: 0.8635\n",
      "Epoch 103/200\n",
      "500/500 [==============================] - 155s 310ms/step - loss: 0.3597 - accuracy: 0.8777 - val_loss: 0.3754 - val_accuracy: 0.8774\n",
      "Epoch 104/200\n",
      "500/500 [==============================] - 146s 292ms/step - loss: 0.3553 - accuracy: 0.8797 - val_loss: 0.3521 - val_accuracy: 0.8836\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 149s 297ms/step - loss: 0.3541 - accuracy: 0.8796 - val_loss: 0.3868 - val_accuracy: 0.8768\n",
      "Epoch 106/200\n",
      "500/500 [==============================] - 150s 301ms/step - loss: 0.3565 - accuracy: 0.8788 - val_loss: 0.5184 - val_accuracy: 0.8403\n",
      "Epoch 107/200\n",
      "500/500 [==============================] - 153s 306ms/step - loss: 0.3500 - accuracy: 0.8817 - val_loss: 0.3954 - val_accuracy: 0.8750\n",
      "Epoch 108/200\n",
      "500/500 [==============================] - 148s 296ms/step - loss: 0.3507 - accuracy: 0.8809 - val_loss: 0.4757 - val_accuracy: 0.8495\n",
      "Epoch 109/200\n",
      "500/500 [==============================] - 147s 294ms/step - loss: 0.3483 - accuracy: 0.8840 - val_loss: 0.4325 - val_accuracy: 0.8641\n",
      "Epoch 110/200\n",
      "500/500 [==============================] - 158s 316ms/step - loss: 0.3446 - accuracy: 0.8832 - val_loss: 0.3852 - val_accuracy: 0.8783\n",
      "Epoch 111/200\n",
      "500/500 [==============================] - 146s 292ms/step - loss: 0.3484 - accuracy: 0.8816 - val_loss: 0.3648 - val_accuracy: 0.8806\n",
      "Epoch 112/200\n",
      "500/500 [==============================] - 148s 297ms/step - loss: 0.3488 - accuracy: 0.8819 - val_loss: 0.3367 - val_accuracy: 0.8909\n",
      "Epoch 113/200\n",
      "500/500 [==============================] - 152s 305ms/step - loss: 0.3477 - accuracy: 0.8816 - val_loss: 0.3290 - val_accuracy: 0.8941\n",
      "Epoch 114/200\n",
      "500/500 [==============================] - 150s 301ms/step - loss: 0.3514 - accuracy: 0.8810 - val_loss: 0.3898 - val_accuracy: 0.8746\n",
      "Epoch 115/200\n",
      "500/500 [==============================] - 146s 291ms/step - loss: 0.3402 - accuracy: 0.8851 - val_loss: 0.3995 - val_accuracy: 0.8765\n",
      "Epoch 116/200\n",
      "500/500 [==============================] - 148s 297ms/step - loss: 0.3510 - accuracy: 0.8820 - val_loss: 0.3706 - val_accuracy: 0.8838\n",
      "Epoch 117/200\n",
      "500/500 [==============================] - 149s 297ms/step - loss: 0.3436 - accuracy: 0.8833 - val_loss: 0.3778 - val_accuracy: 0.8800\n",
      "Epoch 118/200\n",
      "500/500 [==============================] - 153s 306ms/step - loss: 0.3428 - accuracy: 0.8840 - val_loss: 0.3847 - val_accuracy: 0.8799\n",
      "Epoch 119/200\n",
      "500/500 [==============================] - 146s 292ms/step - loss: 0.3458 - accuracy: 0.8829 - val_loss: 0.3704 - val_accuracy: 0.8795\n",
      "Epoch 120/200\n",
      "500/500 [==============================] - 146s 293ms/step - loss: 0.3395 - accuracy: 0.8858 - val_loss: 0.3746 - val_accuracy: 0.8816\n",
      "Epoch 121/200\n",
      "500/500 [==============================] - 150s 300ms/step - loss: 0.3387 - accuracy: 0.8841 - val_loss: 0.3829 - val_accuracy: 0.8809\n",
      "Epoch 122/200\n",
      "500/500 [==============================] - 148s 296ms/step - loss: 0.3391 - accuracy: 0.8843 - val_loss: 0.3999 - val_accuracy: 0.8757\n",
      "Epoch 123/200\n",
      "500/500 [==============================] - 151s 302ms/step - loss: 0.3427 - accuracy: 0.8827 - val_loss: 0.3892 - val_accuracy: 0.8748\n",
      "Epoch 124/200\n",
      "500/500 [==============================] - 146s 292ms/step - loss: 0.3418 - accuracy: 0.8844 - val_loss: 0.3803 - val_accuracy: 0.8823\n",
      "Epoch 125/200\n",
      "500/500 [==============================] - 151s 302ms/step - loss: 0.3377 - accuracy: 0.8854 - val_loss: 0.3477 - val_accuracy: 0.8892\n",
      "Epoch 126/200\n",
      "500/500 [==============================] - 149s 298ms/step - loss: 0.3406 - accuracy: 0.8847 - val_loss: 0.3684 - val_accuracy: 0.8818\n",
      "Epoch 127/200\n",
      "500/500 [==============================] - 151s 302ms/step - loss: 0.3420 - accuracy: 0.8858 - val_loss: 0.3824 - val_accuracy: 0.8762\n",
      "Epoch 128/200\n",
      "500/500 [==============================] - 146s 292ms/step - loss: 0.3368 - accuracy: 0.8844 - val_loss: 0.3465 - val_accuracy: 0.8896\n",
      "Epoch 129/200\n",
      "500/500 [==============================] - 147s 295ms/step - loss: 0.3360 - accuracy: 0.8860 - val_loss: 0.3740 - val_accuracy: 0.8806\n",
      "Epoch 130/200\n",
      "500/500 [==============================] - 154s 309ms/step - loss: 0.3352 - accuracy: 0.8869 - val_loss: 0.3784 - val_accuracy: 0.8801\n",
      "Epoch 131/200\n",
      "500/500 [==============================] - 147s 294ms/step - loss: 0.3386 - accuracy: 0.8844 - val_loss: 0.3461 - val_accuracy: 0.8899\n",
      "Epoch 132/200\n",
      "500/500 [==============================] - 151s 302ms/step - loss: 0.3338 - accuracy: 0.8867 - val_loss: 0.3488 - val_accuracy: 0.8880\n",
      "Epoch 133/200\n",
      "500/500 [==============================] - 149s 299ms/step - loss: 0.3369 - accuracy: 0.8858 - val_loss: 0.3993 - val_accuracy: 0.8793\n",
      "Epoch 134/200\n",
      "500/500 [==============================] - 156s 312ms/step - loss: 0.3375 - accuracy: 0.8852 - val_loss: 0.3407 - val_accuracy: 0.8929\n",
      "Epoch 135/200\n",
      "500/500 [==============================] - 154s 308ms/step - loss: 0.3314 - accuracy: 0.8876 - val_loss: 0.3740 - val_accuracy: 0.8807\n",
      "Epoch 136/200\n",
      "500/500 [==============================] - 160s 319ms/step - loss: 0.3330 - accuracy: 0.8864 - val_loss: 0.3399 - val_accuracy: 0.8923\n",
      "Epoch 137/200\n",
      "500/500 [==============================] - 152s 303ms/step - loss: 0.3295 - accuracy: 0.8871 - val_loss: 0.3345 - val_accuracy: 0.8919\n",
      "Epoch 138/200\n",
      "500/500 [==============================] - 163s 325ms/step - loss: 0.3330 - accuracy: 0.8879 - val_loss: 0.3508 - val_accuracy: 0.8852\n",
      "Epoch 139/200\n",
      "500/500 [==============================] - 151s 302ms/step - loss: 0.3288 - accuracy: 0.8896 - val_loss: 0.3534 - val_accuracy: 0.8849\n",
      "Epoch 140/200\n",
      "500/500 [==============================] - 155s 309ms/step - loss: 0.3223 - accuracy: 0.8909 - val_loss: 0.3709 - val_accuracy: 0.8836\n",
      "Epoch 141/200\n",
      "500/500 [==============================] - 153s 306ms/step - loss: 0.3273 - accuracy: 0.8893 - val_loss: 0.3568 - val_accuracy: 0.8845\n",
      "Epoch 142/200\n",
      "500/500 [==============================] - 157s 313ms/step - loss: 0.3282 - accuracy: 0.8890 - val_loss: 0.3543 - val_accuracy: 0.8857\n",
      "Epoch 143/200\n",
      "500/500 [==============================] - 152s 304ms/step - loss: 0.3245 - accuracy: 0.8899 - val_loss: 0.3607 - val_accuracy: 0.8874\n",
      "Epoch 144/200\n",
      "500/500 [==============================] - 152s 305ms/step - loss: 0.3276 - accuracy: 0.8899 - val_loss: 0.3417 - val_accuracy: 0.8892\n",
      "Epoch 145/200\n",
      "500/500 [==============================] - 160s 319ms/step - loss: 0.3264 - accuracy: 0.8889 - val_loss: 0.3620 - val_accuracy: 0.8871\n",
      "Epoch 146/200\n",
      "500/500 [==============================] - 159s 318ms/step - loss: 0.3265 - accuracy: 0.8910 - val_loss: 0.4414 - val_accuracy: 0.8635\n",
      "Epoch 147/200\n",
      "500/500 [==============================] - 158s 316ms/step - loss: 0.3223 - accuracy: 0.8900 - val_loss: 0.3323 - val_accuracy: 0.8935\n",
      "Epoch 148/200\n",
      "500/500 [==============================] - 176s 353ms/step - loss: 0.3229 - accuracy: 0.8909 - val_loss: 0.3381 - val_accuracy: 0.8923\n",
      "Epoch 149/200\n",
      "500/500 [==============================] - 157s 315ms/step - loss: 0.3220 - accuracy: 0.8916 - val_loss: 0.3476 - val_accuracy: 0.8897\n",
      "Epoch 150/200\n",
      "500/500 [==============================] - 158s 317ms/step - loss: 0.3260 - accuracy: 0.8888 - val_loss: 0.3488 - val_accuracy: 0.8888\n",
      "Epoch 151/200\n",
      "500/500 [==============================] - 152s 304ms/step - loss: 0.3238 - accuracy: 0.8900 - val_loss: 0.3754 - val_accuracy: 0.8816\n",
      "Epoch 152/200\n",
      "500/500 [==============================] - 162s 324ms/step - loss: 0.3247 - accuracy: 0.8900 - val_loss: 0.3395 - val_accuracy: 0.8909\n",
      "Epoch 153/200\n",
      "500/500 [==============================] - 153s 306ms/step - loss: 0.3239 - accuracy: 0.8902 - val_loss: 0.3819 - val_accuracy: 0.8819\n",
      "Epoch 154/200\n",
      "500/500 [==============================] - 183s 366ms/step - loss: 0.3216 - accuracy: 0.8911 - val_loss: 0.5151 - val_accuracy: 0.8438\n",
      "Epoch 155/200\n",
      "500/500 [==============================] - 173s 347ms/step - loss: 0.3212 - accuracy: 0.8924 - val_loss: 0.3347 - val_accuracy: 0.8927\n",
      "Epoch 156/200\n",
      "500/500 [==============================] - 184s 367ms/step - loss: 0.3174 - accuracy: 0.8926 - val_loss: 0.3646 - val_accuracy: 0.8818\n",
      "Epoch 157/200\n",
      "500/500 [==============================] - 173s 347ms/step - loss: 0.3245 - accuracy: 0.8911 - val_loss: 0.3430 - val_accuracy: 0.8924\n",
      "Epoch 158/200\n",
      "500/500 [==============================] - 189s 378ms/step - loss: 0.3183 - accuracy: 0.8916 - val_loss: 0.4324 - val_accuracy: 0.8690\n",
      "Epoch 159/200\n",
      "500/500 [==============================] - 177s 355ms/step - loss: 0.3233 - accuracy: 0.8907 - val_loss: 0.3877 - val_accuracy: 0.8795\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 203s 406ms/step - loss: 0.3175 - accuracy: 0.8929 - val_loss: 0.4731 - val_accuracy: 0.8560\n",
      "Epoch 161/200\n",
      "500/500 [==============================] - 159s 319ms/step - loss: 0.3214 - accuracy: 0.8905 - val_loss: 0.3477 - val_accuracy: 0.8876\n",
      "Epoch 162/200\n",
      "500/500 [==============================] - 163s 326ms/step - loss: 0.3135 - accuracy: 0.8946 - val_loss: 0.3734 - val_accuracy: 0.8825\n",
      "Epoch 163/200\n",
      "500/500 [==============================] - 154s 308ms/step - loss: 0.3203 - accuracy: 0.8919 - val_loss: 0.3700 - val_accuracy: 0.8808\n",
      "Epoch 164/200\n",
      "500/500 [==============================] - 156s 312ms/step - loss: 0.3198 - accuracy: 0.8902 - val_loss: 0.3582 - val_accuracy: 0.8851\n",
      "Epoch 165/200\n",
      "500/500 [==============================] - 167s 334ms/step - loss: 0.3124 - accuracy: 0.8944 - val_loss: 0.4050 - val_accuracy: 0.8739\n",
      "Epoch 166/200\n",
      "500/500 [==============================] - 175s 350ms/step - loss: 0.3149 - accuracy: 0.8915 - val_loss: 0.3616 - val_accuracy: 0.8851\n",
      "Epoch 167/200\n",
      "500/500 [==============================] - 166s 331ms/step - loss: 0.3123 - accuracy: 0.8942 - val_loss: 0.3967 - val_accuracy: 0.8751\n",
      "Epoch 168/200\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.3147 - accuracy: 0.8931 - val_loss: 0.3525 - val_accuracy: 0.8862\n",
      "Epoch 169/200\n",
      "500/500 [==============================] - 160s 320ms/step - loss: 0.3220 - accuracy: 0.8911 - val_loss: 0.3427 - val_accuracy: 0.8894\n",
      "Epoch 170/200\n",
      "500/500 [==============================] - 162s 324ms/step - loss: 0.3111 - accuracy: 0.8956 - val_loss: 0.3221 - val_accuracy: 0.8976\n",
      "Epoch 171/200\n",
      "500/500 [==============================] - 121s 243ms/step - loss: 0.3156 - accuracy: 0.8930 - val_loss: 0.4005 - val_accuracy: 0.8745\n",
      "Epoch 172/200\n",
      "500/500 [==============================] - 109s 218ms/step - loss: 0.3119 - accuracy: 0.8951 - val_loss: 0.3424 - val_accuracy: 0.8881\n",
      "Epoch 173/200\n",
      "500/500 [==============================] - 107s 213ms/step - loss: 0.3131 - accuracy: 0.8941 - val_loss: 0.3784 - val_accuracy: 0.8837\n",
      "Epoch 174/200\n",
      "500/500 [==============================] - 109s 218ms/step - loss: 0.3135 - accuracy: 0.8947 - val_loss: 0.3795 - val_accuracy: 0.8811\n",
      "Epoch 175/200\n",
      "500/500 [==============================] - 107s 214ms/step - loss: 0.3089 - accuracy: 0.8958 - val_loss: 0.3553 - val_accuracy: 0.8893\n",
      "Epoch 176/200\n",
      "500/500 [==============================] - 113s 226ms/step - loss: 0.3086 - accuracy: 0.8968 - val_loss: 0.3736 - val_accuracy: 0.8830\n",
      "Epoch 177/200\n",
      "500/500 [==============================] - 115s 229ms/step - loss: 0.3081 - accuracy: 0.8968 - val_loss: 0.3472 - val_accuracy: 0.8897\n",
      "Epoch 178/200\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 0.3110 - accuracy: 0.8948 - val_loss: 0.3814 - val_accuracy: 0.8792\n",
      "Epoch 179/200\n",
      "500/500 [==============================] - 110s 219ms/step - loss: 0.3126 - accuracy: 0.8949 - val_loss: 0.3509 - val_accuracy: 0.8873\n",
      "Epoch 180/200\n",
      "500/500 [==============================] - 108s 215ms/step - loss: 0.3055 - accuracy: 0.8980 - val_loss: 0.3349 - val_accuracy: 0.8909\n",
      "Epoch 181/200\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 0.3127 - accuracy: 0.8938 - val_loss: 0.3680 - val_accuracy: 0.8839\n",
      "Epoch 182/200\n",
      "500/500 [==============================] - 113s 225ms/step - loss: 0.3121 - accuracy: 0.8954 - val_loss: 0.3320 - val_accuracy: 0.8949\n",
      "Epoch 183/200\n",
      "500/500 [==============================] - 110s 219ms/step - loss: 0.3086 - accuracy: 0.8968 - val_loss: 0.3381 - val_accuracy: 0.8902\n",
      "Epoch 184/200\n",
      "500/500 [==============================] - 108s 217ms/step - loss: 0.3079 - accuracy: 0.8960 - val_loss: 0.3569 - val_accuracy: 0.8858\n",
      "Epoch 185/200\n",
      "500/500 [==============================] - 110s 219ms/step - loss: 0.3104 - accuracy: 0.8939 - val_loss: 0.3603 - val_accuracy: 0.8843\n",
      "Epoch 186/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.3072 - accuracy: 0.8959 - val_loss: 0.3431 - val_accuracy: 0.8917\n",
      "Epoch 187/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.3055 - accuracy: 0.8961 - val_loss: 0.3161 - val_accuracy: 0.8983\n",
      "Epoch 188/200\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 0.3103 - accuracy: 0.8942 - val_loss: 0.3465 - val_accuracy: 0.8896\n",
      "Epoch 189/200\n",
      "500/500 [==============================] - 130s 260ms/step - loss: 0.3043 - accuracy: 0.8970 - val_loss: 0.3236 - val_accuracy: 0.8965\n",
      "Epoch 190/200\n",
      "500/500 [==============================] - 129s 258ms/step - loss: 0.3007 - accuracy: 0.8974 - val_loss: 0.3551 - val_accuracy: 0.8900\n",
      "Epoch 191/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.3059 - accuracy: 0.8954 - val_loss: 0.3523 - val_accuracy: 0.8889\n",
      "Epoch 192/200\n",
      "500/500 [==============================] - 114s 229ms/step - loss: 0.3035 - accuracy: 0.8973 - val_loss: 0.3467 - val_accuracy: 0.8916\n",
      "Epoch 193/200\n",
      "500/500 [==============================] - 128s 257ms/step - loss: 0.3031 - accuracy: 0.8965 - val_loss: 0.3463 - val_accuracy: 0.8921\n",
      "Epoch 194/200\n",
      "500/500 [==============================] - 121s 242ms/step - loss: 0.3036 - accuracy: 0.8965 - val_loss: 0.3571 - val_accuracy: 0.8886\n",
      "Epoch 195/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.3038 - accuracy: 0.8977 - val_loss: 0.3297 - val_accuracy: 0.8952\n",
      "Epoch 196/200\n",
      "500/500 [==============================] - 112s 224ms/step - loss: 0.2997 - accuracy: 0.8983 - val_loss: 0.3218 - val_accuracy: 0.8961\n",
      "Epoch 197/200\n",
      "500/500 [==============================] - 115s 229ms/step - loss: 0.3021 - accuracy: 0.8973 - val_loss: 0.3492 - val_accuracy: 0.8893\n",
      "Epoch 198/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.3008 - accuracy: 0.8982 - val_loss: 0.3430 - val_accuracy: 0.8950\n",
      "Epoch 199/200\n",
      "500/500 [==============================] - 117s 233ms/step - loss: 0.3001 - accuracy: 0.8982 - val_loss: 0.3293 - val_accuracy: 0.8971\n",
      "Epoch 200/200\n",
      "500/500 [==============================] - 132s 264ms/step - loss: 0.2998 - accuracy: 0.8983 - val_loss: 0.3355 - val_accuracy: 0.8963\n",
      "\n",
      "Time for fit =  31824.99\n",
      "LOSS :  0.3\n",
      "VAL_LOSS :  0.336\n",
      "ACCURACY :  0.898\n",
      "VAL_ACCURACY :  0.896\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création des individus (des neurals nets, ici convnet)\n",
    "list_indiv = []\n",
    "for num in range(len(list_indiv_id)):\n",
    "    list_indiv.append(IndividuConvnets(\n",
    "        list_indiv_id[num],\n",
    "          list_epochs[num],\n",
    "          list_batch_size[num],\n",
    "          list_nb_layers[num],\n",
    "          list_l1[num],\n",
    "          list_l2[num],\n",
    "          list_batch_norm[num],\n",
    "          list_dropout[num],\n",
    "          list_filters_per_layers[num],\n",
    "          list_filters_double[num],\n",
    "          list_MLP_end[num],\n",
    "          list_activation[num],\n",
    "          list_kernel[num],\n",
    "          list_padding[num],\n",
    "          list_max_or_avg_pool[num],\n",
    "          list_pool_frequency[num],\n",
    "          list_pool_frequency_change[num],\n",
    "          list_learning_r[num],\n",
    "          list_momentum[num],\n",
    "          list_optimizer[num]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "# Chargement de la classe training, affichag\n",
    "training_1 = MyTraining(1, list_indiv)\n",
    "training_1.all_indiv()\n",
    "training_1.train(main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxpbXOw1RuwM"
   },
   "source": [
    "### Partie tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm6vpk3MRuwQ"
   },
   "outputs": [],
   "source": [
    "# Procedure pour utiliser tensorboard\n",
    "#  1 load la première cell\n",
    "#  2 utiliser la derniere cell avec --logdir (précisez bien votre répertoire, plus sur que ça\n",
    "#    fonctionne avec une string \"mon_path\"\n",
    "#  3 Vous NE POURREZ PLUS update tensorboard sur ce port et il y aura des bugs, pour éviter ça\n",
    "#    quand vous voulez faire une update, fermez jupyter notebook (shutdown total) et réouvrez le \n",
    "#    OU, faites kernel->interrupt et changez de port + de folder de log\n",
    "\n",
    "#si vous voulez tenter de tuer des process\n",
    "#os.system(\"taskkill /im tensorboard.exe /f\") #kill tous les processus qui utilisent tensorboard\n",
    "#os.system('!kill 18776') #kill le processus X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lj738x3Ruwa",
    "outputId": "b7c6a334-5e14-4d18-fc6e-7427bf231326"
   },
   "outputs": [],
   "source": [
    "# Liste des ports utilisés par tensorboard, attention ça se remplit vite et il faut kill jupyter pour clean\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seEJieCtRuwq",
    "outputId": "051fb9d1-b28c-4221-c61c-a8dfbb158362",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code pour démarrer tensorboard dans le dossier souhaité [PRECISEZ BIEN LE DOSSIER ICI]\n",
    "%tensorboard --logdir \"CONVNETS_20200119-0243\\logs_20200119-093909\\tensorboard_data\" --port=6066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upoC4cmYRuwz",
    "outputId": "c0390d77-555a-49f0-934c-b2740b5e2ba0"
   },
   "outputs": [],
   "source": [
    "# Si vous avez la folie des grandeurs\n",
    "notebook.display(port=6066, height=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLuEIeDvRuw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fichier CSV combined_recap + Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO0o33QYs6FP",
    "outputId": "0ebfe25a-2965-4220-ccf7-914ae220328f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commandes pandas utiles\n",
    "data_csv = pd.read_csv(main_directory + \"\\\\combined_recap.csv\")\n",
    "#data_csv = pd.read_csv(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv\")\n",
    "data.head()\n",
    "#meilleure accuracy, moins pire loss par ex\n",
    "#data_csv.sort_values([\"elapsed\"], axis=0, \n",
    "                 #ascending=[False], inplace=True) \n",
    "\n",
    "# Afficher uniquement certaines colonnes\n",
    "#dataX = data_csv.filter(items=['elapsed', 'label'])\n",
    "\n",
    "#récupérer uniquement où la loss est < à X et ou kernel = (3,3) par exemple\n",
    "#dataX = data_csv.loc[(data_csv['elapsed'] > 700) & (data_csv['threadName'].str.contains('Thread Group 1-2'))]\n",
    "#dataX\n",
    "\n",
    "#pd.set_option('display.max_rows', data3.shape[0]+1) #nombre de row max à afficher\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\logs_20200116-204456\\\\recap.csv\")\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGXNwkMCRuxE"
   },
   "outputs": [],
   "source": [
    "image = pyplot.imread(main_directory + \"\\\\logs_20200119-093909\\\\plot.png\")\n",
    "#image = pyplot.imread(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\logs_20200119-093909\\\\plot.png\")\n",
    "pyplot.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "test_several_models_1701_modif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
